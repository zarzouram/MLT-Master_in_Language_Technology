{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Sense Disambiguation using Neural Networks\n",
    "Adam Ek\n",
    "\n",
    "----------------------------------\n",
    "\n",
    "The lab is an exploration and learning exercise to be done in a group and also in discussion with the teachers and other students.\n",
    "\n",
    "Before starting, please read the instructions on [how to work on group assignments](https://github.com/sdobnik/computational-semantics/blob/master/README.md).\n",
    "\n",
    "Write all your answers and the code in the appropriate boxes below.\n",
    "\n",
    "----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A problem with static distributional vectors is the difficulty of distinguishing between different *word senses*. We will continue our exploration of word vectors by considering *trainable vectors* or *word embeddings* for Word Sense Disambiguation (WSD).\n",
    "\n",
    "The goal of word sense disambiguation is to train a model to find the sense of a word (homonyms of a word-form). For example, the word \"bank\" can mean \"sloping land\" or \"financial institution\". \n",
    "\n",
    "(a) \"I deposited my money in the **bank**\" (financial institution)\n",
    "\n",
    "(b) \"I swam from the river **bank**\" (sloping land)\n",
    "\n",
    "In case a) and b) we can determine that the meaning of \"bank\" based on the *context*. To utilize context in a semantic model we use *contextualized word representations*. Previously we worked with *static word representations*, i.e. the representation does not depend on the context. To illustrate we can consider sentences (a) and (b), the word **bank** would have the same static representation in both sentences, which means that it becomes difficult for us to predict its sense. What we want is to create representations that depend on the context, i.e. *contextualized embeddings*. \n",
    "\n",
    "We will create contextualized embeddings with Recurrent Neural Networks. You can read more about recurrent neural netoworks [here](https://colah.github.io/posts/2015-08-Understanding-LSTMs/). Your overall task in this lab is to create a neural network model that can disambiguate the word sense of 15 different words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "available device: cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats as s\n",
    "\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchtext.data import Field, LabelField, TabularDataset, BucketIterator\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "from IPython.core.display import display\n",
    "from IPython.display import Image\n",
    "\n",
    "# Reproducing same results\n",
    "SEED = 2009\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"available device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documenting you code\n",
    "**Note:** This lab is focused quite abit on programming and working with neural networs, i.e. writing code. Comment the code that you write and explain what it does, the code documentation will be taken into account when grading. Also it's very beneficial for you to explain to yourself what the code is doing, and it helps me give feedback to you :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Working with data\n",
    "\n",
    "A central part of any machine learning system is the data we're working with. In this section we will split the data (the dataset is located here: ``wsd-data/wsd_data.txt``) into a training set and a test set. We will also create a baseline to compare our model against. Finally, we will use TorchText to transform our data (raw text) into a convenient format that our neural network can work with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "The dataset we will use contain different word sense for 15 different words. The data is organized as follows (values separated by tabs): \n",
    "- Column 1: word-sense\n",
    "- Column 2: word-form\n",
    "- Column 3: index of word\n",
    "- Column 4: white-space tokenized context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the data\n",
    "\n",
    "Your first task is to seperate the data into a *training set* and a *test set*. The training set should contain 80% of the examples and the test set the remaining 20%. The examples for the test/training set should be selected **randomly**. Save each dataset into a .csv file for loading later. **[2 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done...\n"
     ]
    }
   ],
   "source": [
    "def data_split(path_to_dataset):\n",
    "    # your code goes here\n",
    "    with open (path_to_dataset, \"r\") as fin:\n",
    "        # data = map(lambda x: x.split(\"\\t\"), fin.readlines())\n",
    "        data = map(lambda x: x.split(\"\\t\"), fin.read().splitlines())\n",
    "\n",
    "    # use sklearn to split data reserving the classes ratio in the split dataset.\n",
    "    data = np.array(list(data))\n",
    "    Xtrain, Xtest, ytrain, ytest = train_test_split(\n",
    "            data[:, 1:], data[:, 0], \n",
    "            test_size = 0.2, \n",
    "            stratify = data[:, 0], \n",
    "            random_state = SEED, \n",
    "            shuffle = True\n",
    "            )\n",
    "    \n",
    "    # combine [Xtrain,ytrain] to form train set, the same for Xtest, ytest\n",
    "    trainset = np.concatenate(\n",
    "        (Xtrain, np.array([ytrain]).T), axis=1)\n",
    "    testset = np.concatenate(\n",
    "        (Xtest, np.array([ytest]).T), axis=1)\n",
    "\n",
    "    # Use pandas to write to cv | easier to write :)\n",
    "    colsname = [\"form\", \"id\", \"tokens\", \"sense\"]\n",
    "    trainset = pd.DataFrame(trainset, columns=colsname)\n",
    "    testset = pd.DataFrame(testset, columns=colsname)\n",
    "\n",
    "    trainset.to_csv(\"trainset.csv\", index=False)\n",
    "    testset.to_csv(\"testset.csv\", index=False)\n",
    "    print(\"Done...\")\n",
    "\n",
    "dataset_path = \"./wsd-data/wsd_data.txt\"\n",
    "data_split(dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "--- AE: Marks=2\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a baseline\n",
    "\n",
    "Your second task is to create a *baseline* for the task. A baseline is a \"reality check\" for a model, given a very simple heuristic/algorithmic/model solution to the problem, can our neural network perform better than this?\n",
    "The baseline you are to create is the \"most common sense\" (MCS) baseline. For each word form, find the most commonly assigned sense to the word, and label a words with that sense. **[2 marks]**\n",
    "\n",
    "E.g. In a fictional dataset, \"bank\" have two senses, \"financial institution\" which occur 5 times and \"side of river\" 3 times. Thus, all 8 occurences of bank is labeled \"financial institution\" and this yields an MCS accuracy of 5/8 = 62.5%. If a model obtain a higher score than this, we can conclude that the model *at least* is better than selecting the most frequent word sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>percentage</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>form</th>\n",
       "      <th>sense</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>active.a</td>\n",
       "      <td>active%3:00:03::</td>\n",
       "      <td>0.320596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>bad.a</td>\n",
       "      <td>bad%5:00:00:intense:00</td>\n",
       "      <td>0.607220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>bring.v</td>\n",
       "      <td>bring%2:38:00::</td>\n",
       "      <td>0.211529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>build.v</td>\n",
       "      <td>build%2:36:00::</td>\n",
       "      <td>0.212030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>case.n</td>\n",
       "      <td>case%1:11:00::</td>\n",
       "      <td>0.203822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td rowspan=\"3\" valign=\"top\">common.a</td>\n",
       "      <td>common%3:00:01::</td>\n",
       "      <td>0.250716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>common%3:00:02::</td>\n",
       "      <td>0.250716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>common%5:00:00:shared:00</td>\n",
       "      <td>0.250716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>critical.a</td>\n",
       "      <td>critical%3:00:01::</td>\n",
       "      <td>0.274541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>extend.v</td>\n",
       "      <td>extend%2:30:01::</td>\n",
       "      <td>0.180428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>find.v</td>\n",
       "      <td>find%2:40:02::</td>\n",
       "      <td>0.232013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>follow.v</td>\n",
       "      <td>follow%2:38:00::</td>\n",
       "      <td>0.145944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>force.n</td>\n",
       "      <td>force%1:07:01::</td>\n",
       "      <td>0.162506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>hold.v</td>\n",
       "      <td>hold%2:36:00::</td>\n",
       "      <td>0.151818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>keep.v</td>\n",
       "      <td>keep%2:41:03::</td>\n",
       "      <td>0.391980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>lead.v</td>\n",
       "      <td>lead%2:38:01::</td>\n",
       "      <td>0.179500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>life.n</td>\n",
       "      <td>life%1:26:01::</td>\n",
       "      <td>0.224621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>line.n</td>\n",
       "      <td>line%1:04:01::</td>\n",
       "      <td>0.851217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>major.a</td>\n",
       "      <td>major%3:00:06::</td>\n",
       "      <td>0.302905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>national.a</td>\n",
       "      <td>national%3:00:00::</td>\n",
       "      <td>0.204545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>order.n</td>\n",
       "      <td>order%1:10:03::</td>\n",
       "      <td>0.219451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>physical.a</td>\n",
       "      <td>physical%3:00:00::</td>\n",
       "      <td>0.236304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>place.n</td>\n",
       "      <td>place%1:15:00::</td>\n",
       "      <td>0.242718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>point.n</td>\n",
       "      <td>point%1:09:01::</td>\n",
       "      <td>0.355556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>position.n</td>\n",
       "      <td>position%1:15:00::</td>\n",
       "      <td>0.201463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>positive.a</td>\n",
       "      <td>positive%3:00:01::</td>\n",
       "      <td>0.354209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>professional.a</td>\n",
       "      <td>professional%3:00:01::</td>\n",
       "      <td>0.217717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>regular.a</td>\n",
       "      <td>regular%5:00:00:standard:02</td>\n",
       "      <td>0.217226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>security.n</td>\n",
       "      <td>security%1:26:00::</td>\n",
       "      <td>0.203233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>see.v</td>\n",
       "      <td>see%2:31:00::</td>\n",
       "      <td>0.627653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>serve.v</td>\n",
       "      <td>serve%2:42:03::</td>\n",
       "      <td>0.155052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>time.n</td>\n",
       "      <td>time%1:11:00::</td>\n",
       "      <td>0.278727</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            percentage\n",
       "form           sense                                  \n",
       "active.a       active%3:00:03::               0.320596\n",
       "bad.a          bad%5:00:00:intense:00         0.607220\n",
       "bring.v        bring%2:38:00::                0.211529\n",
       "build.v        build%2:36:00::                0.212030\n",
       "case.n         case%1:11:00::                 0.203822\n",
       "common.a       common%3:00:01::               0.250716\n",
       "               common%3:00:02::               0.250716\n",
       "               common%5:00:00:shared:00       0.250716\n",
       "critical.a     critical%3:00:01::             0.274541\n",
       "extend.v       extend%2:30:01::               0.180428\n",
       "find.v         find%2:40:02::                 0.232013\n",
       "follow.v       follow%2:38:00::               0.145944\n",
       "force.n        force%1:07:01::                0.162506\n",
       "hold.v         hold%2:36:00::                 0.151818\n",
       "keep.v         keep%2:41:03::                 0.391980\n",
       "lead.v         lead%2:38:01::                 0.179500\n",
       "life.n         life%1:26:01::                 0.224621\n",
       "line.n         line%1:04:01::                 0.851217\n",
       "major.a        major%3:00:06::                0.302905\n",
       "national.a     national%3:00:00::             0.204545\n",
       "order.n        order%1:10:03::                0.219451\n",
       "physical.a     physical%3:00:00::             0.236304\n",
       "place.n        place%1:15:00::                0.242718\n",
       "point.n        point%1:09:01::                0.355556\n",
       "position.n     position%1:15:00::             0.201463\n",
       "positive.a     positive%3:00:01::             0.354209\n",
       "professional.a professional%3:00:01::         0.217717\n",
       "regular.a      regular%5:00:00:standard:02    0.217226\n",
       "security.n     security%1:26:00::             0.203233\n",
       "see.v          see%2:31:00::                  0.627653\n",
       "serve.v        serve%2:42:03::                0.155052\n",
       "time.n         time%1:11:00::                 0.278727"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def mcs_baseline(data):\n",
    "    # import data file using pandas\n",
    "    # data_imported = pd.read_csv(data, index_col=0, sep=\";\")\n",
    "    data_imported = pd.read_csv(data, index_col=0)\n",
    "    # Group and count data by word sense\n",
    "    # Then sum words sense counts for each word form = total number of sense labels for each word forms.\n",
    "    # Then, calculate the percentage for each word sense\n",
    "    # Then, get the max percentage\n",
    "    # Finaly, select the word sense that have the max percentage\n",
    "    groupd_sense = data_imported.groupby([\"form\",\"sense\"])[\"sense\"].count()\n",
    "    groupd_form = groupd_sense.groupby(level = 0).transform(sum)\n",
    "    groupd_perc = groupd_sense / groupd_form\n",
    "    groupd_perc_max = groupd_perc.groupby(level = 0).transform(max)\n",
    "    baseline = groupd_perc[groupd_perc==groupd_perc_max].reset_index(name=\"percentage\")\n",
    "    \n",
    "    baseline = baseline.set_index(['form', \"sense\"])   # tranform the \"form\" column to be the index of dataframe\n",
    "    return baseline\n",
    "\n",
    "traindata_path = \"./trainset.csv\"\n",
    "baseline = mcs_baseline(traindata_path)\n",
    "display(baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "--- AE: Marks=2\n",
    "    \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating data iterators\n",
    "\n",
    "To train a neural network, we first need to prepare the data. This involves converting words (and labels) to a number, and organizing the data into batches. We also want the ability to shuffle the examples such that they appear in a random order.  \n",
    "\n",
    "To do all of this we will use the torchtext library (https://torchtext.readthedocs.io/en/latest/index.html). In addition to converting our data into numerical form and creating batches, it will generate a word and label vocabulary, and data iterators than can sort and shuffle the examples. \n",
    "\n",
    "Your task is to create a dataloader for the training and test set you created previously. So, how do we go about doing this?\n",
    "\n",
    "1) First we create a ``Field`` for each of our columns. A field is a function which tokenize the input, keep a dictionary of word-to-numbers, and fix paddings. So, we need four fields, one for the word-sense, one for the position, one for the lemma and one for the context. \n",
    "\n",
    "2) After we have our fields, we need to process the data. For this we use the ``TabularDataset`` class. We pass the name and path of the training and test files we created previously, then we assign which field to use in each column. The result is that each column will be processed by the field indicated. So, the context column will be tokenized and processed by the context field and so on. \n",
    "\n",
    "3) After we have processed the dataset we need to build the vocabulary, for this we call the function ``build_vocab()`` on the different ``Fields`` with the output from ``TabularDataset`` as input. This looks at our dataset and creates the necessary vocabularies (word-to-number mappings). \n",
    "\n",
    "4) Finally, the last step. In the last step we load the data objects given by the ``TabularDataset`` and pass it to the ``BucketIterator`` class. This class will organize our examples into batches and shuffle them around (such that for each epoch the model observe the examples in a different order). When we are done with this we can let our function return the data iterators and vocabularies, then we are ready to train and test our model!\n",
    "\n",
    "Implement the dataloader. [**4 marks**]\n",
    "\n",
    "*hint: for TabularDataset and BucketIterator use the class function splits()* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mytokenizer(text):\n",
    "    return text.split()\n",
    "\n",
    "\n",
    "def dataloader(path, batch_sizes):\n",
    "    print(\"Loading data ...\\n\")\n",
    "    # contexts = Field(tokenize=mytokenizer, eos_token=\"<eos>\",\n",
    "    #                  init_token=\"<bos>\", batch_first=True, include_lengths=True)\n",
    "    sentences = Field(  tokenize=mytokenizer,\n",
    "                        include_lengths=True, \n",
    "                        batch_first=True\n",
    "                      )\n",
    "\n",
    "    senses = Field( batch_first=True,\n",
    "                    sequential=False,\n",
    "                    is_target=True\n",
    "                    )\n",
    "\n",
    "    word_id = Field(batch_first=True, \n",
    "                    sequential=False,\n",
    "                    use_vocab=False, \n",
    "                    dtype=torch.long)\n",
    "\n",
    "    word_f = Field( batch_first=True, \n",
    "                    sequential=False,\n",
    "                   )\n",
    "\n",
    "    # read only required columns – question and label\n",
    "    fields = [(\"form\", word_f), (\"id\", word_id), ('tokens', sentences),\n",
    "              ('senses', senses)]\n",
    "\n",
    "    train_ds, test_ds = TabularDataset.splits(\n",
    "        path=path,\n",
    "        train='trainset.csv', validation=\"testset.csv\",\n",
    "        format='csv',\n",
    "        # csv_reader_params={\"delimiter\": \";\"},\n",
    "        skip_header=True,\n",
    "        fields=fields)\n",
    "\n",
    "    sentences.build_vocab(train_ds, test_ds, \n",
    "                          min_freq=3, vectors=\"glove.6B.300d\")\n",
    "    senses.build_vocab(train_ds, test_ds)\n",
    "    word_id.build_vocab(train_ds, test_ds)\n",
    "    word_f.build_vocab(train_ds, test_ds)\n",
    "\n",
    "    train_iterator, test_iterator = BucketIterator.splits(\n",
    "        (train_ds, test_ds),\n",
    "        batch_size=batch_sizes,\n",
    "        sort_key = lambda x: len(x.tokens),\n",
    "        sort_within_batch=True,\n",
    "        # sort=False,\n",
    "        shuffle=True,\n",
    "        device=device)\n",
    "    \n",
    "    print(\"Loading data done.\\n\")\n",
    "\n",
    "    return train_iterator, test_iterator, sentences, senses, word_id, word_f\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "--- AE: Marks=4\n",
    "    \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Creating and running a Neural Network for WSD\n",
    "\n",
    "In this section we will create and run a neural network to predict word senses based on *contextualized representations*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "We will use a bidirectional Long-Short-Term Memory (LSTM) network to create a representation for the sentences and a Linear classifier to predict the sense of each word.\n",
    "\n",
    "When we initialize the model, we need a few things:\n",
    "\n",
    "    1) An embedding layer: a dictionary from which we can obtain word embeddings\n",
    "    2) A LSTM-module to obtain contextual representations\n",
    "    3) A classifier that compute scores for each word-sense given *some* input\n",
    "\n",
    "\n",
    "The general procedure is the following:\n",
    "\n",
    "    1) For each word in the sentence, obtain word embeddings\n",
    "    2) Run the embedded sentences through the RNN\n",
    "    3) Select the appropriate hidden state\n",
    "    4) Predict the word-sense \n",
    "\n",
    "**Suggestion for efficiency:**  *Use a low dimensionality (32) for word embeddings and the LSTM when developing and testing the code, then scale up when running the full training/tests*\n",
    "    \n",
    "Your tasks will be to create two different models (both follow the two outlines described above), described below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We have a question if taking the last hidden layer of a batch of sentences that have variable lengths is an accurate representation of a word. The last set of tokens is padded to equalize the lengths of sentences. We want to examine the performance when we have the nth hidden vectors, where n is the length of the respective sentence in the batch. See the figure below for the architecture of all methods we implement in this assignment. The `pack_padded_sequence` enables us to do this easily as the generated lat hidden layer is equal to the hidden layers that we want to extract. \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/juliaklezl/lt2213-lab-1-group-3/master/final/image/Architecture1.PNG?token=AOROR3KJSI3X7FSQXA5C5OK675V6M\" width=\"800\" />\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first approach to WSD, you are to select the index of our target word (column 3 in the dataset) and predict the word sense. **[8 marks]**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_intr_output(tensors, indx, x_len, n_dir):\n",
    "    # Extract tesnsors at required indecies (indx)\n",
    "    # the passed tensors dimensions [ batch_size x max_seq_length x n_dir x hidden_dim ]\n",
    "    # indx is a tensor of size 1 x batch_size\n",
    "\n",
    "    # Get words positons for backward sequence, if required\n",
    "    indx_back = (x_len-1) - indx\n",
    "\n",
    "    hidden_dim = tensors.shape[3]\n",
    "    # forward output or final output\n",
    "    # Extract the forward output at first batch then build on it\n",
    "    tensors_final = tensors[0, indx[0], 0, :].contiguous().view(\n",
    "        1, hidden_dim)  # batch=0 [1xhidden_dim]\n",
    "\n",
    "    # Extract backward output at first batch then concatenate it with the forward output @batch=0 [tensors_final]\n",
    "    if n_dir == 2:\n",
    "        tnsr_back_b0 = tensors[0, indx_back[0], 1, :].contiguous().view(\n",
    "            1, hidden_dim)  # [1xhidden_dim]\n",
    "        tensors_final = torch.cat((tensors_final, tnsr_back_b0), dim=1)\n",
    "\n",
    "    # build upon output of batch 0, batch_size=tensors.shape[0]\n",
    "    for i in range(1, tensors.shape[0]):\n",
    "        # Extract forward output or final output at batch=i\n",
    "        tnsr_forward = tensors[i, indx[i], 0, :].contiguous().view(\n",
    "            1, hidden_dim)  # [1xhidden_dim]\n",
    "\n",
    "        # Extract backward output at batch=i then concatenate it with the forward output\n",
    "        if n_dir == 2:\n",
    "            tnsr_back = tensors[0, indx_back[i], 1, :].contiguous().view(\n",
    "                1, hidden_dim)  # [1xhidden_dim]\n",
    "            tensor_temp = torch.cat((tnsr_forward, tnsr_back), dim=1)\n",
    "            # build on old tensors\n",
    "            tensors_final = torch.cat((tensors_final, tensor_temp), dim=0)\n",
    "\n",
    "        else:\n",
    "            tensors_final = torch.cat((tensors_final, tnsr_forward), dim=0)\n",
    "\n",
    "    return tensors_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WSDModel_approach1(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout):\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.n_dir = 2 if bidirectional else 1\n",
    "\n",
    "        # Constructor\n",
    "        super().__init__()\n",
    "\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.LSTM(embedding_dim,\n",
    "                           hidden_dim,\n",
    "                           num_layers=n_layers,\n",
    "                           bidirectional=bidirectional,\n",
    "                           dropout=dropout,\n",
    "                           batch_first=True)\n",
    "        self.classifier = nn.Linear(hidden_dim*self.n_dir, output_dim)\n",
    "\n",
    "    def forward(self, batch):\n",
    "\n",
    "        x, x_len = batch.tokens   # Contexts, and lengthes of each sentence\n",
    "        word_pos = batch.id      # target word indecies\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # embedded size = b x seq x embd\n",
    "        embedded = self.embeddings(x)\n",
    "        \n",
    "#         h_0 = torch.zeros(self.n_layers*self.n_dir, batch_size,\n",
    "#                           self.hidden_dim).requires_grad_()\n",
    "#         # # Initialize cell state\n",
    "#         c_0 = torch.zeros(self.n_layers*self.n_dir, batch_size,\n",
    "#                           self.hidden_dim).requires_grad_()\n",
    "#         h_0 = h_0.to(device)\n",
    "#         c_0 = c_0.to(device)\n",
    "        \n",
    "        # hidden size = l*dir x b x h\n",
    "        # cell   size = l*dir x b x h\n",
    "        # out    size = b x max(seq) x h*dir\n",
    "#         lstm_out, (_, _) = self.rnn(embedded, (h_0.detach(), c_0.detach()))\n",
    "        lstm_out, (_, _) = self.rnn(embedded)\n",
    "    \n",
    "        #out = lstm_out[torch.arange(lstm_out.size(0)),word_pos,:]\n",
    "\n",
    "        # change lstm output dimention to be b x max(seq) x dir x h\n",
    "        seq_length = lstm_out.shape[1]\n",
    "        lstm_out = lstm_out.contiguous().view(batch_size, seq_length, \n",
    "                                 self.n_dir, self.hidden_dim)\n",
    "        # get intermediate outputs that corresponding indecies\n",
    "        out = get_intr_output(lstm_out, word_pos, x_len, self.n_dir)\n",
    "        \n",
    "        # Fully connected layer\n",
    "        predictions = self.classifier(out)\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "--- AE: A tip, you can easily extract the concatenation at the indices with the following line of code: `out = lstm_out[torch.arange(lstm_out.size(0)),word_pos,:]`. So, what this code does is first to select each batch by `torch.arange(lstm_out.size(0))` then, for each batch it selects the index given by `word_pos`. Marks=8\n",
    "    \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the second approach to WSD, you are to predict the word sense based on the final hidden state given by the RNN. **[8 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WSDModel_approach2(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout):\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.n_dir = 2 if bidirectional else 1\n",
    "\n",
    "        # Constructor\n",
    "        super().__init__()\n",
    "\n",
    "        # your code goes here\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        self.rnn = nn.LSTM(embedding_dim,\n",
    "                           hidden_dim,\n",
    "                           num_layers=n_layers,\n",
    "                           bidirectional=bidirectional,\n",
    "                           dropout=dropout,\n",
    "                           batch_first=True)\n",
    "\n",
    "        self.classifier = nn.Linear(hidden_dim*self.n_dir, output_dim)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        x, x_len = batch.tokens    # Contexts, and lengthes of each sentence\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # embedded size = b x seq x embd\n",
    "        embedded = self.embeddings(x)\n",
    "\n",
    "        \n",
    "        # hidden size = l*dir x b x h\n",
    "        # cell   size = l*dir x b x h\n",
    "        # out    size = b x max(seq) x h*dir\n",
    "        _, (hidden, _) = self.rnn(embedded)\n",
    "\n",
    "        # concat final hidden layers\n",
    "        # either forward or backward final hidden layer\n",
    "        hidden_final = hidden[-1, :, :]\n",
    "        # concat the final forward and backward hidden state\n",
    "        if self.n_dir == 2:\n",
    "                out = torch.cat((hidden[-2, :, :], hidden_final), dim=1)\n",
    "\n",
    "        # hidden = [batch size, hid dim * num directions]\n",
    "        predictions = self.classifier(out)\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WSDModel_approach3(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout):\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.n_dir = 2 if bidirectional else 1\n",
    "\n",
    "        # Constructor\n",
    "        super().__init__()\n",
    "\n",
    "        # your code goes here\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        self.rnn = nn.LSTM(embedding_dim,\n",
    "                           hidden_dim,\n",
    "                           num_layers=n_layers,\n",
    "                           bidirectional=bidirectional,\n",
    "                           dropout=dropout,\n",
    "                           batch_first=True)\n",
    "\n",
    "        self.classifier = nn.Linear(hidden_dim*self.n_dir, output_dim)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        x, x_len = batch.tokens    # Contexts, and lengthes of each sentence\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # embedded size = b x seq x embd\n",
    "        embedded = self.embeddings(x)\n",
    "\n",
    "        packed_embedded = pack_padded_sequence( embedded,\n",
    "                                                x_len,\n",
    "                                                batch_first=True)\n",
    "\n",
    "        # hidden size = l*dir x b x h\n",
    "        # cell   size = l*dir x b x h\n",
    "        # out    size = b x max(seq) x h*dir\n",
    "        _, (hidden, _) = self.rnn(packed_embedded)\n",
    "\n",
    "        # concat final hidden layers\n",
    "        # either forward or backward final hidden layer\n",
    "        hidden_final = hidden[-1, :, :]\n",
    "        # concat the final forward and backward hidden state\n",
    "        if self.n_dir == 2:\n",
    "                out = torch.cat((hidden[-2, :, :], hidden_final), dim=1)\n",
    "\n",
    "        # hidden = [batch size, hid dim * num directions]\n",
    "        predictions = self.classifier(out)\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "--- AE: Marks=8\n",
    "    \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and testing the model\n",
    "\n",
    "Now we are ready to train and test our model. What we need now is a loss function, an optimizer, and our data. \n",
    "\n",
    "- First, create the loss function and the optimizer.\n",
    "- Next, we iterate over the number of epochs (i.e. how many times we let the model see our data). \n",
    "- For each epoch, iterate over the dataset (``train_iter``) to obtain batches. Use the batch as input to the model, and let the model output scores for the different word senses.\n",
    "- For each model output, calculate the loss (and print the loss) on the output and update the model parameters.\n",
    "- Reset the gradients and repeat.\n",
    "- After all epochs are done, test your trained model on the test set (``test_iter``) and calculate the total and per-word-form accuracy of your model.\n",
    "\n",
    "Implement the training and testing of the model **[4 marks]**\n",
    "\n",
    "**Suggestion for efficiency:** *when developing your model, try training and testing the model on one or two batches (for each epoch) of data to make sure everything works! It's very annoying if you train for N epochs to find out that something went wrong when testing the model, or to find that something goes wrong when moving from epoch 0 to epoch 1.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, epochs=3, lr=0.01, approach_flag=False):\n",
    "    # Define Loss, Optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    criterion = criterion.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0\n",
    "        model.train()\n",
    "\n",
    "        for i, batch in enumerate(iterator):\n",
    "            \n",
    "            # resets the gradients after every batch\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # forward + backward + optimize\n",
    "            outputs = model(batch)\n",
    "            \n",
    "            # predictions = model(batch)\n",
    "            loss = criterion(outputs, batch.senses)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # loss\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if i % 300 == 299:\n",
    "                print('[%d, %5d] loss: %.3f' %\n",
    "                      (epoch+1, i+1, running_loss / 300))\n",
    "                running_loss = 0.0\n",
    "    return model\n",
    "\n",
    "\n",
    "def test(model, iterator, approach_flag=False):\n",
    "    predictions = []    # list all predictions\n",
    "    predictions_forms = {}  # dict to hold predictions and label per word\n",
    "    labels = []\n",
    "    \n",
    "    # deactivating dropout layers\n",
    "    model.eval()\n",
    "    # deactivates autograd\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            outputs = model(batch)\n",
    "\n",
    "            # probability dist over word senses\n",
    "            probabilities_dist = F.softmax(outputs, dim=0)\n",
    "            # get the word sense prediction\n",
    "            senses_index = torch.max(probabilities_dist, dim=1)[1]\n",
    "            \n",
    "            predictions = predictions + senses_index.tolist()\n",
    "            labels = labels + batch.senses.tolist()\n",
    "            \n",
    "            #print(predictions)\n",
    "            #print(labels)\n",
    "            #assert False\n",
    "\n",
    "            # save prediction for each word form in a dict\n",
    "            myword_form_ids = batch.form.tolist()\n",
    "            # get word form\n",
    "            myword_forms = [word_form.vocab.itos[i] for i in myword_form_ids]\n",
    "\n",
    "            # update dict\n",
    "            for i, wf_id in enumerate(myword_forms):\n",
    "                if wf_id in predictions_forms:\n",
    "                    predictions_forms.get(wf_id, []).append(\n",
    "                        [senses_index.tolist()[i], batch.senses.tolist()[i]])\n",
    "                else:\n",
    "                    predictions_forms[wf_id] = \\\n",
    "                        [[senses_index.tolist()[i], batch.senses.tolist()[i]]]\n",
    "\n",
    "    return predictions, labels, predictions_forms\n",
    "\n",
    "\n",
    "def evaluate(ys, yspredict, ys_forms):\n",
    "    # Calculate overall accuracies and perfromance measures given predictions and labels\n",
    "    accuracy = accuracy_score(ys, yspredict)\n",
    "    report = classification_report(ys, yspredict, output_dict=True)[\n",
    "        \"weighted avg\"]\n",
    "    # store the overall perfromnce report\n",
    "    summary_o = [accuracy, report[\"precision\"],\n",
    "               report[\"recall\"], report[\"f1-score\"]]\n",
    "    \n",
    "    index_df = []\n",
    "    summary = []\n",
    "    # Calculate  accuracies and perfromance measures given predictions and labels for each word form\n",
    "    for f in ys_forms:\n",
    "        # extract columns from 2D array [y, y_prediction] --> [y], [y_prediction]\n",
    "        yspredict = list(map(lambda x, : x[0], ys_forms[f]))\n",
    "        ys = list(map(lambda x, : x[1], ys_forms[f]))\n",
    "        \n",
    "        accuracy = accuracy_score(ys, yspredict)\n",
    "        report = classification_report(ys, yspredict, output_dict=True)[\n",
    "            \"weighted avg\"]\n",
    "        summary.append([accuracy, report[\"precision\"],\n",
    "               report[\"recall\"], report[\"f1-score\"]])\n",
    "        index_df.append(f)\n",
    "    # append the overall perfromance report\n",
    "    summary.append(summary_o); index_df.append(\"Overall\")\n",
    "\n",
    "    # Put performace measure in a dataframe\n",
    "    columns_name = [\"accuracy\", \"precision\", \"recall\", \"F-measure\"]\n",
    "    summary_df = pd.DataFrame(summary, index=index_df, columns=columns_name)\n",
    "\n",
    "    return summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data ...\n",
      "\n",
      "Loading data done.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# loading training and test iterators\n",
    "path_to_folder = \"./\"\n",
    "train_iter, test_iter, mycontexts, mysense_lables, _, word_form = dataloader(\n",
    "    path_to_folder, 32)\n",
    "\n",
    "# define hyperparameters\n",
    "pretrained_embeddings = mycontexts.vocab.vectors\n",
    "size_of_vocab = len(mycontexts.vocab)          # input size of embedding layer\n",
    "# output size of the fully connected layer = number of word senses we have\n",
    "output_size = len(mysense_lables.vocab)\n",
    "# output size of embedding layer = size of words vector\n",
    "embedding_dim = pretrained_embeddings.shape[1]\n",
    "hidden_size = 300#(embedding_dim // 3) + 1         # LSTM hidden layer size\n",
    "num_layers = 2                                 # Number of stacked lstms\n",
    "bidirection = True                             # lstm directions\n",
    "dropout = 0\n",
    "lr = 0.001\n",
    "epochs = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approach 1: Predict the word sense based on word position.\n",
      "\n",
      "-------------------------------------------------- \n",
      "\n",
      "\n",
      "WSDModel_approach1(\n",
      "  (embeddings): Embedding(36442, 300)\n",
      "  (rnn): LSTM(300, 300, num_layers=2, batch_first=True, bidirectional=True)\n",
      "  (classifier): Linear(in_features=600, out_features=223, bias=True)\n",
      ")\n",
      "\n",
      "Training the Model ...\n",
      "\n",
      "[1,   300] loss: 2.222\n",
      "[1,   600] loss: 1.439\n",
      "[1,   900] loss: 1.301\n",
      "[1,  1200] loss: 1.298\n",
      "[1,  1500] loss: 1.287\n",
      "[1,  1800] loss: 1.273\n",
      "[2,   300] loss: 1.230\n",
      "[2,   600] loss: 1.213\n",
      "[2,   900] loss: 1.206\n",
      "[2,  1200] loss: 1.211\n",
      "[2,  1500] loss: 1.218\n",
      "[2,  1800] loss: 1.182\n",
      "[3,   300] loss: 1.094\n",
      "[3,   600] loss: 1.121\n",
      "[3,   900] loss: 1.135\n",
      "[3,  1200] loss: 1.130\n",
      "[3,  1500] loss: 1.107\n",
      "[3,  1800] loss: 1.148\n",
      "Training finished.\n",
      "\n",
      "\n",
      "Testing the Model ...\n",
      "\n",
      "Testing finished\n",
      "\n",
      "\n",
      "---------------------------Performance Measures----------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>F-measure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>force.n</td>\n",
       "      <td>72.55%</td>\n",
       "      <td>73.84%</td>\n",
       "      <td>72.55%</td>\n",
       "      <td>72.71%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>follow.v</td>\n",
       "      <td>41.58%</td>\n",
       "      <td>42.21%</td>\n",
       "      <td>41.58%</td>\n",
       "      <td>40.98%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>place.n</td>\n",
       "      <td>46.89%</td>\n",
       "      <td>59.46%</td>\n",
       "      <td>46.89%</td>\n",
       "      <td>51.93%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>physical.a</td>\n",
       "      <td>47.11%</td>\n",
       "      <td>54.71%</td>\n",
       "      <td>47.11%</td>\n",
       "      <td>49.65%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>bad.a</td>\n",
       "      <td>55.62%</td>\n",
       "      <td>64.09%</td>\n",
       "      <td>55.62%</td>\n",
       "      <td>58.58%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>position.n</td>\n",
       "      <td>39.86%</td>\n",
       "      <td>45.80%</td>\n",
       "      <td>39.86%</td>\n",
       "      <td>40.21%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>bring.v</td>\n",
       "      <td>35.67%</td>\n",
       "      <td>46.90%</td>\n",
       "      <td>35.67%</td>\n",
       "      <td>39.09%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>keep.v</td>\n",
       "      <td>42.63%</td>\n",
       "      <td>63.79%</td>\n",
       "      <td>42.63%</td>\n",
       "      <td>48.93%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>security.n</td>\n",
       "      <td>70.37%</td>\n",
       "      <td>76.07%</td>\n",
       "      <td>70.37%</td>\n",
       "      <td>72.59%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>line.n</td>\n",
       "      <td>45.27%</td>\n",
       "      <td>90.28%</td>\n",
       "      <td>45.27%</td>\n",
       "      <td>57.50%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>professional.a</td>\n",
       "      <td>66.33%</td>\n",
       "      <td>66.15%</td>\n",
       "      <td>66.33%</td>\n",
       "      <td>65.63%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>find.v</td>\n",
       "      <td>43.87%</td>\n",
       "      <td>54.79%</td>\n",
       "      <td>43.87%</td>\n",
       "      <td>48.09%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>time.n</td>\n",
       "      <td>46.46%</td>\n",
       "      <td>59.92%</td>\n",
       "      <td>46.46%</td>\n",
       "      <td>51.73%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>see.v</td>\n",
       "      <td>34.00%</td>\n",
       "      <td>68.94%</td>\n",
       "      <td>34.00%</td>\n",
       "      <td>43.04%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>critical.a</td>\n",
       "      <td>46.50%</td>\n",
       "      <td>57.77%</td>\n",
       "      <td>46.50%</td>\n",
       "      <td>48.09%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>major.a</td>\n",
       "      <td>43.71%</td>\n",
       "      <td>50.53%</td>\n",
       "      <td>43.71%</td>\n",
       "      <td>41.65%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>positive.a</td>\n",
       "      <td>44.63%</td>\n",
       "      <td>50.82%</td>\n",
       "      <td>44.63%</td>\n",
       "      <td>45.29%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>life.n</td>\n",
       "      <td>51.99%</td>\n",
       "      <td>62.25%</td>\n",
       "      <td>51.99%</td>\n",
       "      <td>54.69%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>point.n</td>\n",
       "      <td>49.35%</td>\n",
       "      <td>55.62%</td>\n",
       "      <td>49.35%</td>\n",
       "      <td>51.14%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>common.a</td>\n",
       "      <td>45.69%</td>\n",
       "      <td>49.23%</td>\n",
       "      <td>45.69%</td>\n",
       "      <td>46.46%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>case.n</td>\n",
       "      <td>29.03%</td>\n",
       "      <td>40.16%</td>\n",
       "      <td>29.03%</td>\n",
       "      <td>30.64%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>serve.v</td>\n",
       "      <td>44.79%</td>\n",
       "      <td>48.17%</td>\n",
       "      <td>44.79%</td>\n",
       "      <td>43.54%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>hold.v</td>\n",
       "      <td>36.28%</td>\n",
       "      <td>40.79%</td>\n",
       "      <td>36.28%</td>\n",
       "      <td>37.92%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>order.n</td>\n",
       "      <td>58.75%</td>\n",
       "      <td>60.47%</td>\n",
       "      <td>58.75%</td>\n",
       "      <td>59.21%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>extend.v</td>\n",
       "      <td>41.63%</td>\n",
       "      <td>42.99%</td>\n",
       "      <td>41.63%</td>\n",
       "      <td>41.25%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>regular.a</td>\n",
       "      <td>40.00%</td>\n",
       "      <td>46.21%</td>\n",
       "      <td>40.00%</td>\n",
       "      <td>42.55%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>national.a</td>\n",
       "      <td>55.35%</td>\n",
       "      <td>59.05%</td>\n",
       "      <td>55.35%</td>\n",
       "      <td>55.42%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>build.v</td>\n",
       "      <td>27.00%</td>\n",
       "      <td>32.45%</td>\n",
       "      <td>27.00%</td>\n",
       "      <td>26.28%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>lead.v</td>\n",
       "      <td>31.37%</td>\n",
       "      <td>37.91%</td>\n",
       "      <td>31.37%</td>\n",
       "      <td>32.86%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>active.a</td>\n",
       "      <td>58.74%</td>\n",
       "      <td>64.75%</td>\n",
       "      <td>58.74%</td>\n",
       "      <td>60.22%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Overall</td>\n",
       "      <td>45.00%</td>\n",
       "      <td>55.00%</td>\n",
       "      <td>45.00%</td>\n",
       "      <td>47.03%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                accuracy  precision  recall  F-measure\n",
       "force.n           72.55%     73.84%  72.55%     72.71%\n",
       "follow.v          41.58%     42.21%  41.58%     40.98%\n",
       "place.n           46.89%     59.46%  46.89%     51.93%\n",
       "physical.a        47.11%     54.71%  47.11%     49.65%\n",
       "bad.a             55.62%     64.09%  55.62%     58.58%\n",
       "position.n        39.86%     45.80%  39.86%     40.21%\n",
       "bring.v           35.67%     46.90%  35.67%     39.09%\n",
       "keep.v            42.63%     63.79%  42.63%     48.93%\n",
       "security.n        70.37%     76.07%  70.37%     72.59%\n",
       "line.n            45.27%     90.28%  45.27%     57.50%\n",
       "professional.a    66.33%     66.15%  66.33%     65.63%\n",
       "find.v            43.87%     54.79%  43.87%     48.09%\n",
       "time.n            46.46%     59.92%  46.46%     51.73%\n",
       "see.v             34.00%     68.94%  34.00%     43.04%\n",
       "critical.a        46.50%     57.77%  46.50%     48.09%\n",
       "major.a           43.71%     50.53%  43.71%     41.65%\n",
       "positive.a        44.63%     50.82%  44.63%     45.29%\n",
       "life.n            51.99%     62.25%  51.99%     54.69%\n",
       "point.n           49.35%     55.62%  49.35%     51.14%\n",
       "common.a          45.69%     49.23%  45.69%     46.46%\n",
       "case.n            29.03%     40.16%  29.03%     30.64%\n",
       "serve.v           44.79%     48.17%  44.79%     43.54%\n",
       "hold.v            36.28%     40.79%  36.28%     37.92%\n",
       "order.n           58.75%     60.47%  58.75%     59.21%\n",
       "extend.v          41.63%     42.99%  41.63%     41.25%\n",
       "regular.a         40.00%     46.21%  40.00%     42.55%\n",
       "national.a        55.35%     59.05%  55.35%     55.42%\n",
       "build.v           27.00%     32.45%  27.00%     26.28%\n",
       "lead.v            31.37%     37.91%  31.37%     32.86%\n",
       "active.a          58.74%     64.75%  58.74%     60.22%\n",
       "Overall           45.00%     55.00%  45.00%     47.03%"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# build, train, then test the models\n",
    "# Model Aprroach 1\n",
    "model_1 = WSDModel_approach1(size_of_vocab, embedding_dim, hidden_size,\n",
    "                             output_size, num_layers, bidirectional=bidirection, dropout=dropout)\n",
    "\n",
    "print(\"Approach 1: Predict the word sense based on word position.\\n\")\n",
    "print(\"-\"*50, \"\\n\\n\")\n",
    "\n",
    "print(model_1)\n",
    "print()\n",
    "\n",
    "print(\"Training the Model ...\\n\")\n",
    "# initialize weights of embeddings layers\n",
    "model_1.embeddings.weight.data.copy_(pretrained_embeddings)\n",
    "model_1 = model_1.to(device)    # train the model\n",
    "model_1 = train(model_1, train_iter, epochs)\n",
    "print(\"Training finished.\\n\\n\")\n",
    "\n",
    "print(\"Testing the Model ...\\n\")\n",
    "# Tetss the NN\n",
    "results_1, labels_1, results_dict_1 = test(model_1, test_iter)\n",
    "# Calculate the performance measures\n",
    "results_df1 = evaluate(labels_1, results_1, results_dict_1)\n",
    "results_df1 = results_df1*100\n",
    "print(\"Testing finished\\n\\n\")\n",
    "\n",
    "print(\"{:-^75}\".format(\"Performance Measures\"))\n",
    "# display summary\n",
    "pd.options.display.float_format = '{:.2f}%'.format\n",
    "display(results_df1)\n",
    "# send model_1 to cpu to clear memoru on cuda\n",
    "model_1 = model_1.cpu()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approach 2: Predict the word sense based last hidden layer of lstm.\n",
      "\n",
      "-------------------------------------------------- \n",
      "\n",
      "\n",
      "WSDModel_approach2(\n",
      "  (embeddings): Embedding(36442, 50)\n",
      "  (rnn): LSTM(50, 50, num_layers=2, batch_first=True, bidirectional=True)\n",
      "  (classifier): Linear(in_features=100, out_features=223, bias=True)\n",
      ")\n",
      "\n",
      "Training the Model ...\n",
      "\n",
      "[1,   300] loss: 4.991\n",
      "[1,   600] loss: 4.659\n",
      "[1,   900] loss: 3.777\n",
      "[2,   300] loss: 3.043\n",
      "[2,   600] loss: 2.632\n",
      "[2,   900] loss: 2.382\n",
      "[3,   300] loss: 2.039\n",
      "[3,   600] loss: 1.959\n",
      "[3,   900] loss: 1.936\n",
      "Training finished.\n",
      "\n",
      "\n",
      "Testing the Model ...\n",
      "Testing finished\n",
      "\n",
      "\n",
      "---------------------------Performance Measures----------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>F-measure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>common.a</td>\n",
       "      <td>33.05%</td>\n",
       "      <td>45.67%</td>\n",
       "      <td>33.05%</td>\n",
       "      <td>37.80%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>positive.a</td>\n",
       "      <td>20.66%</td>\n",
       "      <td>62.26%</td>\n",
       "      <td>20.66%</td>\n",
       "      <td>29.97%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>bring.v</td>\n",
       "      <td>19.04%</td>\n",
       "      <td>36.70%</td>\n",
       "      <td>19.04%</td>\n",
       "      <td>22.67%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>case.n</td>\n",
       "      <td>12.92%</td>\n",
       "      <td>29.69%</td>\n",
       "      <td>12.92%</td>\n",
       "      <td>16.24%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>serve.v</td>\n",
       "      <td>37.69%</td>\n",
       "      <td>42.13%</td>\n",
       "      <td>37.69%</td>\n",
       "      <td>38.44%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>line.n</td>\n",
       "      <td>30.39%</td>\n",
       "      <td>89.09%</td>\n",
       "      <td>30.39%</td>\n",
       "      <td>43.23%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>follow.v</td>\n",
       "      <td>27.16%</td>\n",
       "      <td>36.94%</td>\n",
       "      <td>27.16%</td>\n",
       "      <td>28.84%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>find.v</td>\n",
       "      <td>28.48%</td>\n",
       "      <td>53.38%</td>\n",
       "      <td>28.48%</td>\n",
       "      <td>36.05%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>see.v</td>\n",
       "      <td>27.12%</td>\n",
       "      <td>68.56%</td>\n",
       "      <td>27.12%</td>\n",
       "      <td>36.23%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>position.n</td>\n",
       "      <td>30.63%</td>\n",
       "      <td>45.48%</td>\n",
       "      <td>30.63%</td>\n",
       "      <td>35.64%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>hold.v</td>\n",
       "      <td>31.14%</td>\n",
       "      <td>43.69%</td>\n",
       "      <td>31.14%</td>\n",
       "      <td>34.34%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>bad.a</td>\n",
       "      <td>43.23%</td>\n",
       "      <td>62.89%</td>\n",
       "      <td>43.23%</td>\n",
       "      <td>49.93%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>time.n</td>\n",
       "      <td>13.21%</td>\n",
       "      <td>33.15%</td>\n",
       "      <td>13.21%</td>\n",
       "      <td>17.48%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>major.a</td>\n",
       "      <td>19.21%</td>\n",
       "      <td>35.25%</td>\n",
       "      <td>19.21%</td>\n",
       "      <td>23.63%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>order.n</td>\n",
       "      <td>41.50%</td>\n",
       "      <td>53.89%</td>\n",
       "      <td>41.50%</td>\n",
       "      <td>45.23%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>security.n</td>\n",
       "      <td>36.34%</td>\n",
       "      <td>59.32%</td>\n",
       "      <td>36.34%</td>\n",
       "      <td>43.61%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>force.n</td>\n",
       "      <td>41.45%</td>\n",
       "      <td>54.58%</td>\n",
       "      <td>41.45%</td>\n",
       "      <td>45.91%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>point.n</td>\n",
       "      <td>27.42%</td>\n",
       "      <td>49.51%</td>\n",
       "      <td>27.42%</td>\n",
       "      <td>31.15%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>place.n</td>\n",
       "      <td>36.27%</td>\n",
       "      <td>54.93%</td>\n",
       "      <td>36.27%</td>\n",
       "      <td>41.58%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>physical.a</td>\n",
       "      <td>30.79%</td>\n",
       "      <td>41.74%</td>\n",
       "      <td>30.79%</td>\n",
       "      <td>33.50%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>keep.v</td>\n",
       "      <td>38.46%</td>\n",
       "      <td>61.93%</td>\n",
       "      <td>38.46%</td>\n",
       "      <td>46.38%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>professional.a</td>\n",
       "      <td>57.11%</td>\n",
       "      <td>62.11%</td>\n",
       "      <td>57.11%</td>\n",
       "      <td>57.91%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>critical.a</td>\n",
       "      <td>30.89%</td>\n",
       "      <td>41.84%</td>\n",
       "      <td>30.89%</td>\n",
       "      <td>33.23%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>life.n</td>\n",
       "      <td>36.30%</td>\n",
       "      <td>55.24%</td>\n",
       "      <td>36.30%</td>\n",
       "      <td>41.61%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>regular.a</td>\n",
       "      <td>32.41%</td>\n",
       "      <td>43.25%</td>\n",
       "      <td>32.41%</td>\n",
       "      <td>34.70%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>extend.v</td>\n",
       "      <td>35.92%</td>\n",
       "      <td>44.38%</td>\n",
       "      <td>35.92%</td>\n",
       "      <td>37.57%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>national.a</td>\n",
       "      <td>19.30%</td>\n",
       "      <td>31.73%</td>\n",
       "      <td>19.30%</td>\n",
       "      <td>22.99%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>build.v</td>\n",
       "      <td>14.80%</td>\n",
       "      <td>26.92%</td>\n",
       "      <td>14.80%</td>\n",
       "      <td>16.55%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>lead.v</td>\n",
       "      <td>22.75%</td>\n",
       "      <td>37.31%</td>\n",
       "      <td>22.75%</td>\n",
       "      <td>27.72%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>active.a</td>\n",
       "      <td>49.81%</td>\n",
       "      <td>64.18%</td>\n",
       "      <td>49.81%</td>\n",
       "      <td>54.05%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Overall</td>\n",
       "      <td>30.71%</td>\n",
       "      <td>39.76%</td>\n",
       "      <td>30.71%</td>\n",
       "      <td>31.59%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                accuracy  precision  recall  F-measure\n",
       "common.a          33.05%     45.67%  33.05%     37.80%\n",
       "positive.a        20.66%     62.26%  20.66%     29.97%\n",
       "bring.v           19.04%     36.70%  19.04%     22.67%\n",
       "case.n            12.92%     29.69%  12.92%     16.24%\n",
       "serve.v           37.69%     42.13%  37.69%     38.44%\n",
       "line.n            30.39%     89.09%  30.39%     43.23%\n",
       "follow.v          27.16%     36.94%  27.16%     28.84%\n",
       "find.v            28.48%     53.38%  28.48%     36.05%\n",
       "see.v             27.12%     68.56%  27.12%     36.23%\n",
       "position.n        30.63%     45.48%  30.63%     35.64%\n",
       "hold.v            31.14%     43.69%  31.14%     34.34%\n",
       "bad.a             43.23%     62.89%  43.23%     49.93%\n",
       "time.n            13.21%     33.15%  13.21%     17.48%\n",
       "major.a           19.21%     35.25%  19.21%     23.63%\n",
       "order.n           41.50%     53.89%  41.50%     45.23%\n",
       "security.n        36.34%     59.32%  36.34%     43.61%\n",
       "force.n           41.45%     54.58%  41.45%     45.91%\n",
       "point.n           27.42%     49.51%  27.42%     31.15%\n",
       "place.n           36.27%     54.93%  36.27%     41.58%\n",
       "physical.a        30.79%     41.74%  30.79%     33.50%\n",
       "keep.v            38.46%     61.93%  38.46%     46.38%\n",
       "professional.a    57.11%     62.11%  57.11%     57.91%\n",
       "critical.a        30.89%     41.84%  30.89%     33.23%\n",
       "life.n            36.30%     55.24%  36.30%     41.61%\n",
       "regular.a         32.41%     43.25%  32.41%     34.70%\n",
       "extend.v          35.92%     44.38%  35.92%     37.57%\n",
       "national.a        19.30%     31.73%  19.30%     22.99%\n",
       "build.v           14.80%     26.92%  14.80%     16.55%\n",
       "lead.v            22.75%     37.31%  22.75%     27.72%\n",
       "active.a          49.81%     64.18%  49.81%     54.05%\n",
       "Overall           30.71%     39.76%  30.71%     31.59%"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "model_2 = WSDModel_approach2(size_of_vocab, embedding_dim, hidden_size,\n",
    "                             output_size, num_layers, bidirectional=bidirection, dropout=dropout)\n",
    "\n",
    "print(\"Approach 2: Predict the word sense based last hidden layer of lstm.\\n\")\n",
    "print(\"-\"*50, \"\\n\\n\")\n",
    "\n",
    "print(model_2)\n",
    "print()\n",
    "\n",
    "print(\"Training the Model ...\\n\")\n",
    "# initialize weights of embeddings layers\n",
    "model_2.embeddings.weight.data.copy_(pretrained_embeddings)\n",
    "model_2 = model_2.to(device)    # train the model\n",
    "model_2 = train(model_2, train_iter, epochs)\n",
    "print(\"Training finished.\\n\\n\")\n",
    "\n",
    "print(\"Testing the Model ...\")\n",
    "# Tetss the NN\n",
    "results_2, labels_2, results_dict_2 = test(model_2, test_iter)\n",
    "# Calculate the performance measures\n",
    "results_df2 = evaluate(labels_2, results_2, results_dict_2)\n",
    "results_df2 = results_df2*100\n",
    "print(\"Testing finished\\n\\n\")\n",
    "\n",
    "print(\"{:-^75}\".format(\"Performance Measures\"))\n",
    "# display summary\n",
    "pd.options.display.float_format = '{:.2f}%'.format\n",
    "display(results_df2)\n",
    "# send model_1 to cpu to clear memoru on cuda\n",
    "model_2 = model_2.cpu()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approach 3: Predict the word sense based hidden layer of last word of the sequence neglegtin the padding.\n",
      "\n",
      "-------------------------------------------------- \n",
      "\n",
      "\n",
      "WSDModel_approach3(\n",
      "  (embeddings): Embedding(36442, 50)\n",
      "  (rnn): LSTM(50, 50, num_layers=2, batch_first=True, bidirectional=True)\n",
      "  (classifier): Linear(in_features=100, out_features=223, bias=True)\n",
      ")\n",
      "\n",
      "Training the Model ...\n",
      "\n",
      "[1,   300] loss: 4.891\n",
      "[1,   600] loss: 3.876\n",
      "[1,   900] loss: 3.045\n",
      "[2,   300] loss: 2.410\n",
      "[2,   600] loss: 2.158\n",
      "[2,   900] loss: 1.998\n",
      "[3,   300] loss: 1.650\n",
      "[3,   600] loss: 1.629\n",
      "[3,   900] loss: 1.607\n",
      "Training finished.\n",
      "\n",
      "\n",
      "Testing the Model ...\n",
      "Testing finished\n",
      "\n",
      "\n",
      "---------------------------Performance Measures----------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>F-measure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>common.a</td>\n",
       "      <td>32.47%</td>\n",
       "      <td>45.63%</td>\n",
       "      <td>32.47%</td>\n",
       "      <td>37.53%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>positive.a</td>\n",
       "      <td>48.35%</td>\n",
       "      <td>64.00%</td>\n",
       "      <td>48.35%</td>\n",
       "      <td>54.36%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>bring.v</td>\n",
       "      <td>24.85%</td>\n",
       "      <td>40.72%</td>\n",
       "      <td>24.85%</td>\n",
       "      <td>28.91%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>case.n</td>\n",
       "      <td>17.16%</td>\n",
       "      <td>31.92%</td>\n",
       "      <td>17.16%</td>\n",
       "      <td>21.45%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>serve.v</td>\n",
       "      <td>39.67%</td>\n",
       "      <td>50.19%</td>\n",
       "      <td>39.67%</td>\n",
       "      <td>41.90%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>line.n</td>\n",
       "      <td>43.37%</td>\n",
       "      <td>91.52%</td>\n",
       "      <td>43.37%</td>\n",
       "      <td>56.79%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>follow.v</td>\n",
       "      <td>26.25%</td>\n",
       "      <td>36.34%</td>\n",
       "      <td>26.25%</td>\n",
       "      <td>29.51%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>find.v</td>\n",
       "      <td>31.19%</td>\n",
       "      <td>52.05%</td>\n",
       "      <td>31.19%</td>\n",
       "      <td>38.51%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>see.v</td>\n",
       "      <td>30.63%</td>\n",
       "      <td>69.86%</td>\n",
       "      <td>30.63%</td>\n",
       "      <td>41.19%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>position.n</td>\n",
       "      <td>36.71%</td>\n",
       "      <td>52.55%</td>\n",
       "      <td>36.71%</td>\n",
       "      <td>42.40%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>hold.v</td>\n",
       "      <td>33.07%</td>\n",
       "      <td>46.12%</td>\n",
       "      <td>33.07%</td>\n",
       "      <td>37.48%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>bad.a</td>\n",
       "      <td>49.57%</td>\n",
       "      <td>70.92%</td>\n",
       "      <td>49.57%</td>\n",
       "      <td>56.77%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>time.n</td>\n",
       "      <td>13.21%</td>\n",
       "      <td>29.09%</td>\n",
       "      <td>13.21%</td>\n",
       "      <td>17.01%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>major.a</td>\n",
       "      <td>29.14%</td>\n",
       "      <td>41.23%</td>\n",
       "      <td>29.14%</td>\n",
       "      <td>32.75%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>order.n</td>\n",
       "      <td>42.75%</td>\n",
       "      <td>59.26%</td>\n",
       "      <td>42.75%</td>\n",
       "      <td>48.83%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>security.n</td>\n",
       "      <td>43.52%</td>\n",
       "      <td>68.48%</td>\n",
       "      <td>43.52%</td>\n",
       "      <td>50.90%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>force.n</td>\n",
       "      <td>46.73%</td>\n",
       "      <td>67.68%</td>\n",
       "      <td>46.73%</td>\n",
       "      <td>54.26%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>point.n</td>\n",
       "      <td>41.51%</td>\n",
       "      <td>53.18%</td>\n",
       "      <td>41.51%</td>\n",
       "      <td>45.15%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>place.n</td>\n",
       "      <td>39.38%</td>\n",
       "      <td>60.96%</td>\n",
       "      <td>39.38%</td>\n",
       "      <td>46.79%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>physical.a</td>\n",
       "      <td>40.79%</td>\n",
       "      <td>56.57%</td>\n",
       "      <td>40.79%</td>\n",
       "      <td>44.44%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>keep.v</td>\n",
       "      <td>44.67%</td>\n",
       "      <td>70.85%</td>\n",
       "      <td>44.67%</td>\n",
       "      <td>53.40%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>professional.a</td>\n",
       "      <td>58.10%</td>\n",
       "      <td>66.36%</td>\n",
       "      <td>58.10%</td>\n",
       "      <td>60.45%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>critical.a</td>\n",
       "      <td>32.48%</td>\n",
       "      <td>44.48%</td>\n",
       "      <td>32.48%</td>\n",
       "      <td>36.20%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>life.n</td>\n",
       "      <td>43.56%</td>\n",
       "      <td>69.13%</td>\n",
       "      <td>43.56%</td>\n",
       "      <td>51.34%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>regular.a</td>\n",
       "      <td>40.25%</td>\n",
       "      <td>50.29%</td>\n",
       "      <td>40.25%</td>\n",
       "      <td>44.09%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>extend.v</td>\n",
       "      <td>43.06%</td>\n",
       "      <td>48.08%</td>\n",
       "      <td>43.06%</td>\n",
       "      <td>43.88%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>national.a</td>\n",
       "      <td>25.35%</td>\n",
       "      <td>41.15%</td>\n",
       "      <td>25.35%</td>\n",
       "      <td>29.93%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>build.v</td>\n",
       "      <td>14.20%</td>\n",
       "      <td>29.79%</td>\n",
       "      <td>14.20%</td>\n",
       "      <td>17.40%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>lead.v</td>\n",
       "      <td>24.71%</td>\n",
       "      <td>38.81%</td>\n",
       "      <td>24.71%</td>\n",
       "      <td>29.04%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>active.a</td>\n",
       "      <td>48.33%</td>\n",
       "      <td>63.68%</td>\n",
       "      <td>48.33%</td>\n",
       "      <td>54.19%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Overall</td>\n",
       "      <td>35.87%</td>\n",
       "      <td>44.96%</td>\n",
       "      <td>35.87%</td>\n",
       "      <td>37.55%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                accuracy  precision  recall  F-measure\n",
       "common.a          32.47%     45.63%  32.47%     37.53%\n",
       "positive.a        48.35%     64.00%  48.35%     54.36%\n",
       "bring.v           24.85%     40.72%  24.85%     28.91%\n",
       "case.n            17.16%     31.92%  17.16%     21.45%\n",
       "serve.v           39.67%     50.19%  39.67%     41.90%\n",
       "line.n            43.37%     91.52%  43.37%     56.79%\n",
       "follow.v          26.25%     36.34%  26.25%     29.51%\n",
       "find.v            31.19%     52.05%  31.19%     38.51%\n",
       "see.v             30.63%     69.86%  30.63%     41.19%\n",
       "position.n        36.71%     52.55%  36.71%     42.40%\n",
       "hold.v            33.07%     46.12%  33.07%     37.48%\n",
       "bad.a             49.57%     70.92%  49.57%     56.77%\n",
       "time.n            13.21%     29.09%  13.21%     17.01%\n",
       "major.a           29.14%     41.23%  29.14%     32.75%\n",
       "order.n           42.75%     59.26%  42.75%     48.83%\n",
       "security.n        43.52%     68.48%  43.52%     50.90%\n",
       "force.n           46.73%     67.68%  46.73%     54.26%\n",
       "point.n           41.51%     53.18%  41.51%     45.15%\n",
       "place.n           39.38%     60.96%  39.38%     46.79%\n",
       "physical.a        40.79%     56.57%  40.79%     44.44%\n",
       "keep.v            44.67%     70.85%  44.67%     53.40%\n",
       "professional.a    58.10%     66.36%  58.10%     60.45%\n",
       "critical.a        32.48%     44.48%  32.48%     36.20%\n",
       "life.n            43.56%     69.13%  43.56%     51.34%\n",
       "regular.a         40.25%     50.29%  40.25%     44.09%\n",
       "extend.v          43.06%     48.08%  43.06%     43.88%\n",
       "national.a        25.35%     41.15%  25.35%     29.93%\n",
       "build.v           14.20%     29.79%  14.20%     17.40%\n",
       "lead.v            24.71%     38.81%  24.71%     29.04%\n",
       "active.a          48.33%     63.68%  48.33%     54.19%\n",
       "Overall           35.87%     44.96%  35.87%     37.55%"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_3 = WSDModel_approach3(size_of_vocab, embedding_dim, hidden_size,\n",
    "                             output_size, num_layers, bidirectional=bidirection, dropout=dropout)\n",
    "\n",
    "print(\"Approach 3: Predict the word sense based hidden layer of last word of the sequence neglegtin the padding.\\n\")\n",
    "print(\"-\"*50, \"\\n\\n\")\n",
    "\n",
    "print(model_3)\n",
    "print()\n",
    "\n",
    "print(\"Training the Model ...\\n\")\n",
    "# initialize weights of embeddings layers\n",
    "model_3.embeddings.weight.data.copy_(pretrained_embeddings)\n",
    "model_3 = model_3.to(device)    # train the model\n",
    "train(model_3, train_iter, epochs)\n",
    "print(\"Training finished.\\n\\n\")\n",
    "\n",
    "print(\"Testing the Model ...\")\n",
    "# Tetss the NN\n",
    "results_3, labels_3, results_dict_3 = test(model_3, test_iter)\n",
    "# Calculate the performance measures\n",
    "results_df3 = evaluate(labels_3, results_3, results_dict_3)\n",
    "results_df3 = results_df3*100\n",
    "print(\"Testing finished\\n\\n\")\n",
    "\n",
    "print(\"{:-^75}\".format(\"Performance Measures\"))\n",
    "# display summary\n",
    "pd.options.display.float_format = '{:.2f}%'.format\n",
    "display(results_df3)\n",
    "# send model_1 to cpu to clear memoru on cuda\n",
    "model_3 = model_3.cpu()\n",
    "print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tried different sets of **Hyperparameters**; we noticed that the system's overall accuracy decreased with the increase in the embedding layers' output size. We used `Glove` to embed words; the highest accuracy achieved when we used `Glove` with size 50 to embed words while `Glove` with size 300 has the worst accuracy. Also, we noticed that the system accuracy has the same inverse relation with the hidden layer size of LSTM.\n",
    "\n",
    "It seems that the `Glove` representation has noise data that affect system accuracy. Such an adverse effect may be attenuated when the size of the word embedding decreases in both the embedding layer and the LSTM layer; the LSTM reduces the dimensionality of the word embeddings further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "--- AE: Looks good! I liked your third approach, this is generally a good idea to do. As you noticed, it does not typically give a HUGE performance boost (since well, the NN typically learns that the padding is useless), but when optimizing your models this is very recommended!\n",
    "\n",
    "So, typically, larger dimensions (128-256) in this task would yield better performance, but this does not seem to be the case for your system where your analysis holds!\n",
    "\n",
    "Marks=4\n",
    "    \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain the difference between the first and second approach. What kind of representations are the different approaches using to predict word-senses? **[5 marks]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer should go here:**\n",
    "\n",
    "In the first approach, the representation of the specific word in question is used as input to the final prediction layer. So the word sense is chosen based on information about the ambiguous word; its context is encoded in the embedding. We have used bidirectional LSTM so we can also include the contexts that come after the word in the embeddings.\n",
    "\n",
    "In the second approach, the final embedding layer takes the final hidden state of the RNN layer as input. This is the LSTM’s long-term memory, based on the entire sequence. However, due to the fact that the context data was padded in the preprocessing step, this final state also includes semantically irrelevant padding data. Therefore, we added a third approach which follows the same principle as the second model, but excludes the padded datapoints. So the final hidden state refers to the state after the last actual word was processed, without including <pad>’s. \n",
    "\n",
    "So the first approach relies more on the information about the individual word whereas the second (and third) one focuse more on the sentence overall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "--- AE: Good analysis! The second approach is essentially the CBOW approach we did for word2vec in the previous lab. I.e. we look at all the words in a sentence and try to find the best representation given all of them. Marks=5\n",
    "    \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate your model with per-word-form *accuracy* and comment on the results you get, how does the model perform in comparison to the baseline, and how does the first approach compare to the second? Which one is more successful? **[5 marks]**\n",
    "\n",
    "**Your answer should go here:**\n",
    "\n",
    "\n",
    "Overall, the first model performs the best (overall 56% accuracy), followed by our third approach (20.6%), then the second approach (10.6%). \n",
    "\n",
    "### The comments below are related to the new training and evaluation session:\n",
    "\n",
    "The figure below shows that the first approach has the height accuracy for all words form.\n",
    "Only the first model predicts the sense of most of the words with accuracies higher than the baseline, except for one word The other models, model2 and model 3, fail to get accuracies higher than the baseline. See the figure below\n",
    "\n",
    "Also, the two figures, the old and new training sessions, show that the perfromance for the model-1 does not change a lot (relatively to the other models). This consistency indicates that the first approach has a better capability to prdict the the word sense.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/juliaklezl/lt2213-lab-1-group-3/master/final/image/accuraciesperword_new.png?token=AOROR3LIX7VJ2M7CGKLWPUC675XS66\" width=\"800\" />\n",
    "\n",
    "---\n",
    "\n",
    "### The comments below are related to the old training and evaluation session:\n",
    "\n",
    "Comparing Model-2 and Model -3, There is a slight increase in precision (2.5%) relative to the increase in accuracy (1.7%). This small increase may be due to that the embeddings do not include the padding sequence.\n",
    "\n",
    "The figure below shows that the first approach has the height accuracy for all words form. Our three models predict the sense of most of the words with accuracies higher than the baseline, except for two words for model-1 and six words for model2 and model 3.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/juliaklezl/lt2213-lab-1-group-3/master/final/image/accuraciesperword.PNG?token=AOROR3MF7NTLJX2W7ZY5D5S675XQ6\" width=\"800\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "--- AE: Reasonable! Generally, we would expect model 1 to perform better with *this* setup, which it does for you. The code you have implemented for all models is also correct, so I'm abit puzzled as to why the performance of model 2 and 3 is so low.  \n",
    "\n",
    "Marks=5\n",
    "    \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What could we do to improve our word sense disambiguation model? **[8 marks]**\n",
    "\n",
    "**Your answer should go here:**\n",
    "\n",
    "- Add more syntactical data, such as POS or dependency information. This probably gets encoded by the embeddings to some extent, but explicitly adding it might help to disambiguate, since word senses often differ among others in their POS or the POS-tags of words they co-occur with.\n",
    "\n",
    "- Experiment with different parameters, batch sizes, learning rates, different or added layers, different loss, or optimizer functions. Can evolution algorithms help in optimizing these parameters?\n",
    "\n",
    "- More preprocessing of the input data (the context column), e.g., removing stopwords\n",
    "\n",
    "- Investigate applying dimensionality reduction on pre-trained word embeddings, please see our note at the end of question 2\n",
    "\n",
    "- The accuracies for the models changed as the tarining/evaluation data changed, specially for Model-2 and Model-3. To metigate such effect it is prefered to train and evaluate the models over multiple 3 or 5 folds.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "--- AE: Good suggestions! Another thing we should add is some for of regularization to help our model generalize, such as dropout. Marks=7\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "--- AE: 44 marks! Very good job, well written code and reasonable analysis. \n",
    "\n",
    "Hope you have a nice summer!\n",
    "\n",
    "Best, Adam\n",
    "    \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Readings:\n",
    "\n",
    "[1] Kågebäck, M., & Salomonsson, H. (2016). Word Sense Disambiguation using a Bidirectional LSTM. arXiv preprint arXiv:1606.03568.\n",
    "\n",
    "[2] https://cl.lingfil.uu.se/~nivre/master/NLP-LexSem.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
