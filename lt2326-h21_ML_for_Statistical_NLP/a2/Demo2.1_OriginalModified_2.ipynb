{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data, padding (based on 2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### I added Start Of Sentence token `<SOS>` and End Of Sentence token `<EOS>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label data\n",
    "#\n",
    "# Start of Sentence: SOS: 0\n",
    "# End of Sentence:   EOS: 1\n",
    "# Word Start:             2\n",
    "# Word End:               3\n",
    "def read_chinese_data(inputfilename):\n",
    "    with open(inputfilename, \"r\") as inputfile:\n",
    "        sentences = []\n",
    "        collection_words = []\n",
    "        collection_labels = []\n",
    "        for line in inputfile:\n",
    "            if line[0] == '#':\n",
    "                continue\n",
    "            columns = line.split()\n",
    "            if columns == []:\n",
    "                sent = \"<SOS>\" + ''.join(collection_words) + \"<EOS>\"\n",
    "                sentences.append((sent, [0] + collection_labels + [1]))\n",
    "                collection_words = []\n",
    "                collection_labels = []\n",
    "                continue\n",
    "            collection_words.append(columns[1])\n",
    "            collection_labels += [2] + ([3] * (len(columns[1]) - 1))\n",
    "\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = read_chinese_data(\n",
    "    '/scratch/lt2316-h20-resources/zh_gsd-ud-train.conllu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = read_chinese_data(\n",
    "    '/scratch/lt2316-h20-resources/zh_gsd-ud-test.conllu')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `PAD` index is zero, `<EOS>` and `<SOS>` at the end of word to index dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def index_chars(sentences):\n",
    "    regx = r\"(^<SOS>|<EOS><SOS>|<EOS>$)\"  # remove added token from sentences\n",
    "    megasentence = re.sub(regx, \"\", \"\".join(sentences))\n",
    "    char_list = set()\n",
    "    for c in megasentence:\n",
    "        char_list.add(c)\n",
    "    # add EOS and SOS at the end of index list\n",
    "    char_list = [\"PAD\"] + list(char_list) + [\"<EOS>\"] + [\"<SOS>\"]\n",
    "    return char_list, {char_list[x]: x for x in range(len(char_list))}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_index, char_index = index_chars(\n",
    "    [x[0] for x in train_sentences + test_sentences])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PAD 0\n",
      "望 1\n",
      "錳 2\n",
      "遼 3\n",
      "孫 4\n",
      "皋 5\n",
      "談 6\n",
      "下 7\n",
      "嶽 8\n",
      "圃 9\n"
     ]
    }
   ],
   "source": [
    "# int_index: List of char in the train document\n",
    "# char_index: char to index dict; index of a char in int_index.\n",
    "for i in range(10):\n",
    "    print(int_index[i], char_index[int_index[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert sequence of chars to the respective sequence of indecies\n",
    "def convert_sentence(sentence, index):\n",
    "    sent = sentence[5:-5]  # remove start and end tokens then add them\n",
    "    sos = sentence[:5]\n",
    "    eos = sentence[-5:]\n",
    "    return [index[sos]] + [index[x] for x in sent] + [index[eos]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_lengths(sentences, max_length, padding=0):\n",
    "    return [x + ([padding] * (max_length - len(x))) for x in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Text\n",
      "<SOS>活化歷史建築諮詢委員會在2010年9月完成評審建議書，經發展局局長接納及批准，由嘉道理農場暨植物園公司建議的「綠滙學苑」方案獲選。<EOS>\n",
      "\n",
      "Sentence Segmentation Label\n",
      "[0, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 2, 2, 3, 3, 3, 2, 2, 2, 2, 3, 2, 3, 2, 3, 2, 2, 2, 2, 3, 2, 2, 3, 2, 3, 2, 2, 3, 2, 2, 2, 3, 3, 2, 3, 2, 2, 3, 2, 2, 3, 2, 3, 2, 2, 2, 3, 2, 3, 2, 2, 3, 2, 3, 2, 1]\n",
      "\n",
      "Sentence in int\n",
      "[3649, 3412, 1658, 1013, 268, 578, 1624, 2396, 3321, 1024, 63, 1476, 2827, 611, 3023, 1379, 3023, 2890, 1956, 2985, 559, 2772, 2341, 2621, 578, 1450, 3526, 3521, 1952, 1972, 2456, 2829, 2829, 472, 2137, 1258, 1799, 2239, 2284, 3521, 2831, 3230, 525, 2007, 1366, 3551, 1998, 343, 571, 346, 1617, 1790, 578, 1450, 798, 3604, 2480, 2866, 2223, 2893, 407, 1857, 2575, 1284, 1581, 64, 3648]\n",
      "\n",
      "Sentence Padded\n",
      "[[3649, 3412, 1658, 1013, 268, 578, 1624, 2396, 3321, 1024, 63, 1476, 2827, 611, 3023, 1379, 3023, 2890, 1956, 2985, 559, 2772, 2341, 2621, 578, 1450, 3526, 3521, 1952, 1972, 2456, 2829, 2829, 472, 2137, 1258, 1799, 2239, 2284, 3521, 2831, 3230, 525, 2007, 1366, 3551, 1998, 343, 571, 346, 1617, 1790, 578, 1450, 798, 3604, 2480, 2866, 2223, 2893, 407, 1857, 2575, 1284, 1581, 64, 3648, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "rand_id = random.randint(0, len(train_sentences) - 1)\n",
    "sample_sent = train_sentences[rand_id][0]\n",
    "sample_label = train_sentences[rand_id][1]\n",
    "sent_convert = convert_sentence(sample_sent, char_index)\n",
    "\n",
    "print(\"Sentence Text\")\n",
    "print(sample_sent)\n",
    "print()\n",
    "print(\"Sentence Segmentation Label\")\n",
    "print(sample_label)\n",
    "print()\n",
    "print(\"Sentence in int\")\n",
    "print(sent_convert)\n",
    "print()\n",
    "print(\"Sentence Padded\")\n",
    "pad_len = len(sent_convert) + 4\n",
    "print(pad_lengths([sent_convert], pad_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(x, device=\"cpu\"):\n",
    "    converted = [(convert_sentence(x1[0], char_index), x1[1]) for x1 in x]\n",
    "    X, y = zip(*converted)\n",
    "    lengths = [len(x2) for x2 in X]\n",
    "    padded_X = pad_lengths(X, max(lengths))\n",
    "    Xt = torch.LongTensor(padded_X).to(device)\n",
    "    padded_y = pad_lengths(y, max(lengths), padding=-1)\n",
    "    yt = torch.LongTensor(padded_y).to(device)\n",
    "    lengths_t = torch.LongTensor(lengths).to(device)\n",
    "    return Xt, lengths_t, yt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_tensor, train_lengths_tensor, train_y_tensor = create_dataset(\n",
    "    train_sentences, torch.device(\"cuda:1\"))\n",
    "test_X_tensor, test_lengths_tensor, test_y_tensor = create_dataset(\n",
    "    test_sentences, torch.device(\"cuda:1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train X:\n",
      " tensor([3649, 3412, 1658, 1013,  268,  578, 1624, 2396, 3321, 1024,   63, 1476,\n",
      "        2827,  611, 3023, 1379, 3023, 2890, 1956, 2985,  559, 2772, 2341, 2621,\n",
      "         578, 1450, 3526, 3521, 1952, 1972, 2456, 2829, 2829,  472, 2137, 1258,\n",
      "        1799, 2239, 2284, 3521, 2831, 3230,  525, 2007, 1366, 3551, 1998,  343,\n",
      "         571,  346, 1617, 1790,  578, 1450,  798, 3604, 2480, 2866, 2223, 2893,\n",
      "         407, 1857, 2575, 1284, 1581,   64, 3648,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0], device='cuda:1')\n"
     ]
    }
   ],
   "source": [
    "print(\"Train X:\\n\", train_X_tensor[rand_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Y:\n",
      " tensor([ 0,  2,  3,  2,  3,  2,  3,  2,  3,  2,  3,  2,  2,  2,  3,  3,  3,  2,\n",
      "         2,  2,  2,  3,  2,  3,  2,  3,  2,  2,  2,  2,  3,  2,  2,  3,  2,  3,\n",
      "         2,  2,  3,  2,  2,  2,  3,  3,  2,  3,  2,  2,  3,  2,  2,  3,  2,  3,\n",
      "         2,  2,  2,  3,  2,  3,  2,  2,  3,  2,  3,  2,  1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1], device='cuda:1')\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTrain Y:\\n\", train_y_tensor[rand_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batcher:\n",
    "    def __init__(self, X, lengths, y, device, batch_size=50, max_iter=None):\n",
    "        self.X = X\n",
    "        self.lengths = lengths.to(torch.device(\"cpu\"))\n",
    "        self.y = y\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "        self.max_iter = max_iter\n",
    "        self.curr_iter = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.curr_iter == self.max_iter:\n",
    "            raise StopIteration\n",
    "        permutation = torch.randperm(self.X.size()[0], device=self.device)\n",
    "        permX = self.X[permutation]\n",
    "        permlengths = self.lengths[permutation]\n",
    "        permy = self.y[permutation]\n",
    "        splitX = torch.split(permX, self.batch_size)\n",
    "        splitlengths = torch.split(permlengths, self.batch_size)\n",
    "        splity = torch.split(permy, self.batch_size)\n",
    "\n",
    "        self.curr_iter += 1\n",
    "        return splitX, splitlengths, splity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = Batcher(train_X_tensor,\n",
    "                  train_lengths_tensor,\n",
    "                  train_y_tensor,\n",
    "                  torch.device('cuda:1'),\n",
    "                  max_iter=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3997, 184])\n",
      "80 80 80\n",
      "torch.Size([50, 184])\n",
      "torch.Size([50])\n",
      "torch.Size([50, 184])\n"
     ]
    }
   ],
   "source": [
    "X0, l0, y0 = next(batches)\n",
    "print(train_X_tensor.size())\n",
    "print(len(X0), len(l0), len(y0))\n",
    "print(X0[0].size())\n",
    "print(l0[0].size())\n",
    "print(y0[0].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### I Added an Encoder layer (LSTM) under the word segmentation layer\n",
    "##### I Removed the `log_softmax` from the model as I will goint to use `CrossEntropyLoss`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleDualObj(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size, enc_h_size, seg_h_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.emb = nn.Embedding(vocab_size, emb_size, 0)\n",
    "        self.encoder = SeqEncoder(emb_size, enc_h_size)\n",
    "        self.word_seg = Segmenter(enc_h_size, seg_h_size)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "\n",
    "        embs = self.emb(x)\n",
    "        encdd = self.encoder(embs, lengths)\n",
    "        seg_logits = self.word_seg(encdd, lengths)\n",
    "\n",
    "        return seg_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqEncoder(nn.Module):\n",
    "    def __init__(self, input_size, enc_h_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, enc_h_size, batch_first=True)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        packed = pack_padded_sequence(x,\n",
    "                                      lengths,\n",
    "                                      batch_first=True,\n",
    "                                      enforce_sorted=False)\n",
    "        output1, _ = self.lstm(packed)\n",
    "        unpacked, _ = pad_packed_sequence(output1, batch_first=True)\n",
    "\n",
    "        return unpacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Segmenter(nn.Module):\n",
    "    def __init__(self, input_size, seg_h_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, seg_h_size, batch_first=True)\n",
    "        self.sig1 = nn.Tanh()\n",
    "        self.lin = nn.Linear(seg_h_size, 4)\n",
    "\n",
    "    def forward(self, x_embd, lengths):\n",
    "        packed = pack_padded_sequence(x_embd,\n",
    "                                      lengths,\n",
    "                                      batch_first=True,\n",
    "                                      enforce_sorted=False)\n",
    "        output1, _ = self.lstm(packed)\n",
    "        unpacked, _ = pad_packed_sequence(output1, batch_first=True)\n",
    "        output2 = self.sig1(unpacked)\n",
    "        output3 = self.lin(output2)\n",
    "\n",
    "        return output3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X,\n",
    "          lengths,\n",
    "          y,\n",
    "          vocab_size,\n",
    "          emb_size,\n",
    "          enc_h_size,\n",
    "          seg_h_size,\n",
    "          batch_size,\n",
    "          epochs,\n",
    "          device,\n",
    "          model=None):\n",
    "\n",
    "    batches = Batcher(X,\n",
    "                      lengths,\n",
    "                      y,\n",
    "                      device,\n",
    "                      batch_size=batch_size,\n",
    "                      max_iter=epochs)\n",
    "\n",
    "    if not model:\n",
    "        m = SingleDualObj(vocab_size, emb_size, enc_h_size,\n",
    "                          seg_h_size).to(device)\n",
    "    else:\n",
    "        m = model\n",
    "\n",
    "    loss = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "    optimizer = optim.Adam(m.parameters(), lr=0.005)\n",
    "\n",
    "    epoch = 0\n",
    "    for batch in batches:\n",
    "\n",
    "        tot_loss = 0\n",
    "        for X, ls, y in zip(*batch):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            o = m(X, ls)\n",
    "            l = loss(o.permute(0, 2, 1), y[:, :max(ls)])\n",
    "            tot_loss += l.item()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if epoch % 4 == 0:\n",
    "            print(\"Total loss in epoch {:<2d} is {:.3f}.\".format(\n",
    "                epoch, tot_loss))\n",
    "\n",
    "        epoch += 1\n",
    "\n",
    "    print(\"Total loss in epoch {:<2d} is {:.3f}.\".format(epoch - 1, tot_loss))\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\n",
    "    \"vocab_size\": len(int_index),\n",
    "    \"emb_size\": 300,\n",
    "    \"enc_h_size\": 200,\n",
    "    \"seg_h_size\": 150,\n",
    "    \"batch_size\": 50,\n",
    "    \"epochs\": 30,\n",
    "    \"device\": torch.device(\"cuda:1\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss in epoch 0  is 29.987.\n",
      "Total loss in epoch 4  is 3.383.\n",
      "Total loss in epoch 8  is 1.349.\n",
      "Total loss in epoch 12 is 0.726.\n",
      "Total loss in epoch 16 is 0.763.\n",
      "Total loss in epoch 20 is 0.577.\n",
      "Total loss in epoch 24 is 0.539.\n",
      "Total loss in epoch 28 is 0.547.\n",
      "Total loss in epoch 29 is 0.725.\n"
     ]
    }
   ],
   "source": [
    "model = train(train_X_tensor, train_lengths_tensor, train_y_tensor, **kwargs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "preds = []\n",
    "gtruth = []\n",
    "test_lengs = test_lengths_tensor.to(torch.device(\"cpu\"))\n",
    "for i, X_test in enumerate(test_X_tensor):\n",
    "    with torch.no_grad():\n",
    "        output = model(X_test.view(1, -1), test_lengs[i].view(-1))\n",
    "    prediction = torch.argmax(F.log_softmax(output, 2), 2)\n",
    "\n",
    "    preds.extend(prediction[:test_lengs[i]].float().view(-1).tolist())\n",
    "    gtruth.extend(test_y_tensor[i][:test_lengs[i]].float().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00       500\n",
      "         1.0       1.00      1.00      1.00       500\n",
      "         2.0       0.95      0.96      0.95     12012\n",
      "         3.0       0.93      0.91      0.92      7194\n",
      "\n",
      "    accuracy                           0.94     20206\n",
      "   macro avg       0.97      0.97      0.97     20206\n",
      "weighted avg       0.94      0.94      0.94     20206\n",
      "\n",
      "Accuracy: 94.33%\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(gtruth, preds)\n",
    "print(classification_report(gtruth, preds))\n",
    "print(f\"Accuracy: {accuracy:.2%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
