{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data, padding (based on 2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_chinese_data(inputfilename):\n",
    "    with open(inputfilename, \"r\") as inputfile:\n",
    "        sentences = []\n",
    "        collection_words = []\n",
    "        collection_labels = []\n",
    "        for line in inputfile:\n",
    "            if line[0] == '#':\n",
    "                continue\n",
    "            columns = line.split()\n",
    "            #print(words)\n",
    "            if columns == []:\n",
    "                sentences.append(\n",
    "                    (''.join(collection_words), collection_labels))\n",
    "                collection_words = []\n",
    "                collection_labels = []\n",
    "                continue\n",
    "            collection_words.append(columns[1])\n",
    "            collection_labels += [1] + ([0] * (len(columns[1]) - 1))\n",
    "\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = read_chinese_data(\n",
    "    '/scratch/lt2316-h20-resources/zh_gsd-ud-train.conllu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = read_chinese_data(\n",
    "    '/scratch/lt2316-h20-resources/zh_gsd-ud-test.conllu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_chars(sentences):\n",
    "    megasentence = ''.join(sentences)\n",
    "    char_list = set()\n",
    "    for c in megasentence:\n",
    "        char_list.add(c)\n",
    "    char_list = [0] + list(char_list)\n",
    "    return char_list, {char_list[x]: x for x in range(len(char_list))}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_index, char_index = index_chars(\n",
    "    [x[0] for x in train_sentences + test_sentences])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n",
      "靼 1\n",
      "忒 2\n",
      "邵 3\n",
      "酉 4\n",
      "渣 5\n",
      "延 6\n",
      "賃 7\n",
      "辰 8\n",
      "赤 9\n"
     ]
    }
   ],
   "source": [
    "# int_index: List of char in the train document\n",
    "# char_index: char to index dict; index of a char in int_index.\n",
    "for i in range(10):\n",
    "    print(int_index[i], char_index[int_index[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert sequence of chars to the respective sequence of indecies\n",
    "def convert_sentence(sentence, index):\n",
    "    return [index[x] for x in sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_lengths(sentences, max_length, padding=0):\n",
    "    return [x + ([padding] * (max_length - len(x))) for x in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(x, device=\"cpu\"):\n",
    "    converted = [(convert_sentence(x1[0], char_index), x1[1]) for x1 in x]\n",
    "    X, y = zip(*converted)\n",
    "    lengths = [len(x2) for x2 in X]\n",
    "    padded_X = pad_lengths(X, max(lengths))\n",
    "    Xt = torch.LongTensor(padded_X).to(device)\n",
    "    padded_y = pad_lengths(y, max(lengths), padding=-1)\n",
    "    yt = torch.LongTensor(padded_y).to(device)\n",
    "    lengths_t = torch.LongTensor(lengths).to(device)\n",
    "    return Xt, lengths_t, yt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_tensor, train_lengths_tensor, train_y_tensor = create_dataset(\n",
    "    train_sentences, \"cuda:2\")\n",
    "test_X_tensor, test_lengths_tensor, test_y_tensor = create_dataset(\n",
    "    test_sentences, \"cuda:2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train X:\n",
      " tensor([1553,  312, 2836, 1507, 1707,  311, 3311, 1133, 1044,  308, 2221, 2932,\n",
      "        1458, 1707, 2566, 1731, 3214, 1643, 1278, 1109, 1765, 1091, 3311, 2595,\n",
      "        1909, 2339, 1091, 1021,  701, 2774,  324, 1707,  144, 2928, 2024, 2595,\n",
      "         648, 3353, 1091, 1144, 2274, 1707, 2566, 1033, 2253, 3398,  100, 1707,\n",
      "        3557, 1569, 2932,  882, 1091, 2286, 3311, 1967,  843, 1396,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0], device='cuda:2')\n"
     ]
    }
   ],
   "source": [
    "print(\"Train X:\\n\", train_X_tensor[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Y:\n",
      " tensor([ 1,  0,  1,  0,  1,  1,  1,  1,  1,  1,  1,  1,  0,  1,  1,  1,  0,  1,\n",
      "         0,  1,  0,  1,  1,  1,  1,  0,  1,  1,  0,  1,  0,  1,  1,  1,  1,  1,\n",
      "         1,  0,  1,  1,  0,  1,  1,  1,  0,  0,  0,  1,  1,  0,  1,  0,  1,  1,\n",
      "         0,  1,  0,  1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1], device='cuda:2')\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTrain Y:\\n\", train_y_tensor[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batcher:\n",
    "    def __init__(self, X, lengths, y, device, batch_size=50, max_iter=None):\n",
    "        self.X = X\n",
    "        self.lengths = lengths.to(\"cpu\")\n",
    "        self.y = y\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "        self.max_iter = max_iter\n",
    "        self.curr_iter = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.curr_iter == self.max_iter:\n",
    "            raise StopIteration\n",
    "        permutation = torch.randperm(self.X.size()[0], device=self.device)\n",
    "        permX = self.X[permutation]\n",
    "        permlengths = self.lengths[permutation]\n",
    "        permy = self.y[permutation]\n",
    "        splitX = torch.split(permX, self.batch_size)\n",
    "        splitlengths = torch.split(permlengths, self.batch_size)\n",
    "        splity = torch.split(permy, self.batch_size)\n",
    "\n",
    "        self.curr_iter += 1\n",
    "        return splitX, splitlengths, splity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = Batcher(train_X_tensor,\n",
    "                  train_lengths_tensor,\n",
    "                  train_y_tensor,\n",
    "                  torch.device('cuda:2'),\n",
    "                  max_iter=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3997, 182])\n",
      "80 80 80\n",
      "torch.Size([50, 182])\n",
      "torch.Size([50])\n",
      "torch.Size([50, 182])\n"
     ]
    }
   ],
   "source": [
    "X0, l0, y0 = next(batches)\n",
    "print(train_X_tensor.size())\n",
    "print(len(X0), len(l0), len(y0))\n",
    "print(X0[0].size())\n",
    "print(l0[0].size())\n",
    "print(y0[0].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Segmenter(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.emb_size = emb_size\n",
    "\n",
    "        self.emb = nn.Embedding(self.vocab_size, self.emb_size, 0)\n",
    "        self.lstm = nn.LSTM(self.emb_size, 150, batch_first=True)\n",
    "        self.sig1 = nn.Sigmoid()\n",
    "        self.lin = nn.Linear(150, 2)\n",
    "        self.softmax = nn.LogSoftmax(2)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        embs = self.emb(x)\n",
    "        packed = pack_padded_sequence(embs,\n",
    "                                      lengths,\n",
    "                                      batch_first=True,\n",
    "                                      enforce_sorted=False)\n",
    "        output1, _ = self.lstm(packed)\n",
    "        unpacked, _ = pad_packed_sequence(output1, batch_first=True)\n",
    "        output2 = self.sig1(unpacked)\n",
    "        output3 = self.lin(output2)\n",
    "        return self.softmax(output3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X,\n",
    "          lengths,\n",
    "          y,\n",
    "          vocab_size,\n",
    "          emb_size,\n",
    "          batch_size,\n",
    "          epochs,\n",
    "          device,\n",
    "          model=None):\n",
    "\n",
    "    batches = Batcher(X,\n",
    "                      lengths,\n",
    "                      y,\n",
    "                      device,\n",
    "                      batch_size=batch_size,\n",
    "                      max_iter=epochs)\n",
    "\n",
    "    if not model:\n",
    "        m = Segmenter(vocab_size, emb_size).to(device)\n",
    "    else:\n",
    "        m = model\n",
    "\n",
    "    loss = nn.NLLLoss(ignore_index=-1)\n",
    "    optimizer = optim.Adam(m.parameters(), lr=0.005)\n",
    "\n",
    "    epoch = 0\n",
    "    for batch in batches:\n",
    "\n",
    "        tot_loss = 0\n",
    "        for X, ls, y in zip(*batch):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            o = m(X, ls)\n",
    "            l = loss(o.permute(0, 2, 1), y[:, :max(ls)])\n",
    "            tot_loss += l.item()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if epoch % 4 == 0:\n",
    "            print(\"Total loss in epoch {:<2d} is {:.3f}.\".format(\n",
    "                epoch, tot_loss))\n",
    "\n",
    "        epoch += 1\n",
    "\n",
    "    print(\"Total loss in epoch {:<2d} is {:.3f}.\".format(epoch - 1, tot_loss))\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss in epoch 0  is 35.846.\n",
      "Total loss in epoch 4  is 9.246.\n",
      "Total loss in epoch 8  is 4.132.\n",
      "Total loss in epoch 12 is 2.154.\n",
      "Total loss in epoch 16 is 2.076.\n",
      "Total loss in epoch 20 is 1.237.\n",
      "Total loss in epoch 24 is 0.277.\n",
      "Total loss in epoch 28 is 0.129.\n",
      "Total loss in epoch 29 is 0.116.\n"
     ]
    }
   ],
   "source": [
    "model = train(train_X_tensor, train_lengths_tensor, train_y_tensor,\n",
    "              len(int_index), 200, 50, 30, \"cuda:2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "preds = []\n",
    "gtruth = []\n",
    "for i, X_test in enumerate(test_X_tensor):\n",
    "    output = model(X_test.view(1, -1), [test_lengths_tensor[i].item()])\n",
    "    prediction = torch.argmax(output, 2)\n",
    "\n",
    "    preds.extend(prediction[:test_lengths_tensor[i]].float().view(-1).tolist())\n",
    "    gtruth.extend(test_y_tensor[i][:test_lengths_tensor[i]].float().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.89      0.90      7194\n",
      "         1.0       0.93      0.94      0.94     12012\n",
      "\n",
      "    accuracy                           0.92     19206\n",
      "   macro avg       0.92      0.92      0.92     19206\n",
      "weighted avg       0.92      0.92      0.92     19206\n",
      "\n",
      "Accuracy: 92.27%\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(gtruth, preds)\n",
    "print(classification_report(gtruth, preds))\n",
    "print(f\"Accuracy: {accuracy:.2%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
