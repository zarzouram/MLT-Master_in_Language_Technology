{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data, padding (based on 2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### I added Start Of Sentence token `<SOS>` and End Of Sentence token `<EOS>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label data\n",
    "#\n",
    "# Start of Sentence: SOS: 0\n",
    "# End of Sentence:   EOS: 1\n",
    "# Word Start:             2\n",
    "# Word End:               3\n",
    "def read_chinese_data(inputfilename):\n",
    "    with open(inputfilename, \"r\") as inputfile:\n",
    "        sentences = []\n",
    "        collection_words = []\n",
    "        collection_labels = []\n",
    "        for line in inputfile:\n",
    "            if line[0] == '#':\n",
    "                continue\n",
    "            columns = line.split()\n",
    "            if columns == []:\n",
    "                sent = \"<SOS>\" + ''.join(collection_words) + \"<EOS>\"\n",
    "                sentences.append((sent, [0] + collection_labels + [1]))\n",
    "                collection_words = []\n",
    "                collection_labels = []\n",
    "                continue\n",
    "            collection_words.append(columns[1])\n",
    "            collection_labels += [2] + ([3] * (len(columns[1]) - 1))\n",
    "\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = read_chinese_data(\n",
    "    '/scratch/lt2316-h20-resources/zh_gsd-ud-train.conllu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = read_chinese_data(\n",
    "    '/scratch/lt2316-h20-resources/zh_gsd-ud-test.conllu')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `PAD` index is zero, `<EOS>` and `<SOS>` at the end of word to index dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def index_chars(sentences):\n",
    "    regx = r\"(^<SOS>|<EOS><SOS>|<EOS>$)\"\n",
    "    megasentence = re.sub(regx, \"\", \"\".join(sentences))\n",
    "    char_list = set()\n",
    "    for c in megasentence:\n",
    "        char_list.add(c)\n",
    "    char_list = [\"PAD\"] + list(char_list) + [\"<EOS>\"] + [\"<SOS>\"]\n",
    "    return char_list, {char_list[x]: x for x in range(len(char_list))}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_index, char_index = index_chars(\n",
    "    [x[0] for x in train_sentences + test_sentences])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PAD 0\n",
      "妍 1\n",
      "敗 2\n",
      "舉 3\n",
      "蓓 4\n",
      "蜜 5\n",
      "輿 6\n",
      "閻 7\n",
      "悔 8\n",
      "漳 9\n"
     ]
    }
   ],
   "source": [
    "# int_index: List of char in the train document\n",
    "# char_index: char to index dict; index of a char in int_index.\n",
    "for i in range(10):\n",
    "    print(int_index[i], char_index[int_index[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert sequence of chars to the respective sequence of indecies\n",
    "def convert_sentence(sentence, index):\n",
    "    sent = sentence[5:-5]  # remove start and end word\n",
    "    sos = sentence[:5]\n",
    "    eos = sentence[-5:]\n",
    "    return [index[sos]] + [index[x] for x in sent] + [index[eos]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_lengths(sentences, max_length, padding=0):\n",
    "    return [x + ([padding] * (max_length - len(x))) for x in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Text\n",
      "<SOS>索菲兩人逃出來後前往美國大使館受阻，於是躲到巴黎市中心的一個公園。<EOS>\n",
      "\n",
      "Sentence Segmentation Label\n",
      "[0, 2, 3, 2, 2, 2, 2, 3, 2, 2, 3, 2, 3, 2, 3, 2, 2, 3, 2, 2, 3, 2, 3, 2, 3, 2, 2, 3, 2, 2, 2, 2, 3, 2, 1]\n",
      "\n",
      "Sentence in int\n",
      "[3649, 3636, 1511, 170, 3346, 339, 2254, 3552, 1551, 212, 1558, 2408, 458, 1348, 3139, 3478, 118, 1503, 531, 704, 2493, 375, 3105, 1162, 673, 744, 146, 3085, 2705, 1643, 222, 571, 2347, 2544, 3648]\n",
      "\n",
      "Sentence Padded\n",
      "[[3649, 3636, 1511, 170, 3346, 339, 2254, 3552, 1551, 212, 1558, 2408, 458, 1348, 3139, 3478, 118, 1503, 531, 704, 2493, 375, 3105, 1162, 673, 744, 146, 3085, 2705, 1643, 222, 571, 2347, 2544, 3648, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "rand_id = random.randint(0, len(train_sentences) - 1)\n",
    "sample_sent = train_sentences[rand_id][0]\n",
    "sample_label = train_sentences[rand_id][1]\n",
    "sent_convert = convert_sentence(sample_sent, char_index)\n",
    "\n",
    "print(\"Sentence Text\")\n",
    "print(sample_sent)\n",
    "print()\n",
    "print(\"Sentence Segmentation Label\")\n",
    "print(sample_label)\n",
    "print()\n",
    "print(\"Sentence in int\")\n",
    "print(sent_convert)\n",
    "print()\n",
    "print(\"Sentence Padded\")\n",
    "pad_len = len(sent_convert) + 4\n",
    "print(pad_lengths([sent_convert], pad_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(x, device=\"cpu\"):\n",
    "    converted = [(convert_sentence(x1[0], char_index), x1[1]) for x1 in x]\n",
    "    X, y = zip(*converted)\n",
    "    lengths = [len(x2) for x2 in X]\n",
    "    padded_X = pad_lengths(X, max(lengths))\n",
    "    Xt = torch.LongTensor(padded_X).to(device)\n",
    "    padded_y = pad_lengths(y, max(lengths), padding=-1)\n",
    "    yt = torch.LongTensor(padded_y).to(device)\n",
    "    lengths_t = torch.LongTensor(lengths).to(device)\n",
    "    return Xt, lengths_t, yt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_tensor, train_lengths_tensor, train_y_tensor = create_dataset(\n",
    "    train_sentences, torch.device(\"cuda:1\"))\n",
    "test_X_tensor, test_lengths_tensor, test_y_tensor = create_dataset(\n",
    "    test_sentences, torch.device(\"cuda:1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train X:\n",
      " tensor([3649, 3636, 1511,  170, 3346,  339, 2254, 3552, 1551,  212, 1558, 2408,\n",
      "         458, 1348, 3139, 3478,  118, 1503,  531,  704, 2493,  375, 3105, 1162,\n",
      "         673,  744,  146, 3085, 2705, 1643,  222,  571, 2347, 2544, 3648,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0], device='cuda:1')\n"
     ]
    }
   ],
   "source": [
    "print(\"Train X:\\n\", train_X_tensor[rand_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Y:\n",
      " tensor([ 0,  2,  3,  2,  2,  2,  2,  3,  2,  2,  3,  2,  3,  2,  3,  2,  2,  3,\n",
      "         2,  2,  3,  2,  3,  2,  3,  2,  2,  3,  2,  2,  2,  2,  3,  2,  1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1], device='cuda:1')\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTrain Y:\\n\", train_y_tensor[rand_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batcher:\n",
    "    def __init__(self, X, lengths, y, device, batch_size=50, max_iter=None):\n",
    "        self.X = X\n",
    "        self.lengths = lengths.to(torch.device(\"cpu\"))\n",
    "        self.y = y\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "        self.max_iter = max_iter\n",
    "        self.curr_iter = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.curr_iter == self.max_iter:\n",
    "            raise StopIteration\n",
    "        permutation = torch.randperm(self.X.size()[0], device=self.device)\n",
    "        permX = self.X[permutation]\n",
    "        permlengths = self.lengths[permutation]\n",
    "        permy = self.y[permutation]\n",
    "        splitX = torch.split(permX, self.batch_size)\n",
    "        splitlengths = torch.split(permlengths, self.batch_size)\n",
    "        splity = torch.split(permy, self.batch_size)\n",
    "\n",
    "        self.curr_iter += 1\n",
    "        return splitX, splitlengths, splity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = Batcher(train_X_tensor,\n",
    "                  train_lengths_tensor,\n",
    "                  train_y_tensor,\n",
    "                  torch.device('cuda:1'),\n",
    "                  max_iter=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3997, 184])\n",
      "80 80 80\n",
      "torch.Size([50, 184])\n",
      "torch.Size([50])\n",
      "torch.Size([50, 184])\n"
     ]
    }
   ],
   "source": [
    "X0, l0, y0 = next(batches)\n",
    "print(train_X_tensor.size())\n",
    "print(len(X0), len(l0), len(y0))\n",
    "print(X0[0].size())\n",
    "print(l0[0].size())\n",
    "print(y0[0].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### I Added an Encoder layer (LSTM) under the word segmentation layer.\n",
    "##### I Removed the `log_softmax` from the model as I will goint to use `CrossEntropyLoss`.\n",
    "##### I moved the embedding layer to a separate class.\n",
    "##### I added a layer for char generation `self.generator` above the encoder layer.\n",
    "##### I added a layer for word segmentation `self.word_seg` above the encoder layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleDualObj(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size, enc_h_size, seg_h_size,\n",
    "                 gen_h_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = SeqEmbd(vocab_size, emb_size)\n",
    "        self.encoder = SeqEncoder(emb_size, enc_h_size)\n",
    "        self.word_seg = Segmenter(enc_h_size, seg_h_size)\n",
    "        self.generator = CharGenerator(enc_h_size, gen_h_size, vocab_size - 1)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "\n",
    "        embs = self.embedding(x)\n",
    "        encdd = self.encoder(embs, lengths)\n",
    "        gen_trn_seq = encdd.clone()[:, :-1]\n",
    "        gen_logits = self.generator(gen_trn_seq, lengths - 1)\n",
    "        seg_logits = self.word_seg(encdd, lengths)\n",
    "\n",
    "        return gen_logits, seg_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqEmbd(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.emb = nn.Embedding(vocab_size, emb_size, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embs = self.emb(x)\n",
    "\n",
    "        return embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqEncoder(nn.Module):\n",
    "    def __init__(self, input_size, enc_h_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, enc_h_size, batch_first=True)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        packed = pack_padded_sequence(x,\n",
    "                                      lengths,\n",
    "                                      batch_first=True,\n",
    "                                      enforce_sorted=False)\n",
    "        output1, _ = self.lstm(packed)\n",
    "        unpacked, _ = pad_packed_sequence(output1, batch_first=True)\n",
    "\n",
    "        return unpacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Segmenter(nn.Module):\n",
    "    def __init__(self, input_size, seg_h_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, seg_h_size, batch_first=True)\n",
    "        self.fc =  nn.Sequential(nn.Sigmoid(), nn.Linear(seg_h_size, 4))\n",
    "\n",
    "    def forward(self, x_embd, lengths):\n",
    "        packed = pack_padded_sequence(x_embd,\n",
    "                                      lengths,\n",
    "                                      batch_first=True,\n",
    "                                      enforce_sorted=False)\n",
    "        last_out, _ = self.lstm(packed)\n",
    "        unpacked, _ = pad_packed_sequence(last_out, batch_first=True)\n",
    "        logits = self.fc(unpacked)\n",
    "\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The char generation layer consists of an LSTM layer and a classifier consisting of three Linear layers with a `Tanh` activation function in between. I normalized the output of the LSTM layer, the first and second Linear layers to enhance the performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharGenerator(nn.Module):\n",
    "    def __init__(self, input_size, gen_h_size, vocab_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.sequence = nn.LSTM(input_size,\n",
    "                                gen_h_size,\n",
    "                                num_layers=1,\n",
    "                                batch_first=True)\n",
    "\n",
    "        h_size1 = (gen_h_size + vocab_size) // 2\n",
    "        h_size2 = (h_size1 + vocab_size) // 3\n",
    "        self.clf1 = nn.Linear(gen_h_size, h_size1)\n",
    "        self.clf2 = nn.Linear(h_size1, h_size2)\n",
    "        self.fc = nn.Linear(h_size2, vocab_size)\n",
    "\n",
    "        self.bn_enc = nn.BatchNorm1d(gen_h_size)\n",
    "        self.bn_clf1 = nn.BatchNorm1d(h_size1)\n",
    "        self.bn_clf2 = nn.BatchNorm1d(h_size2)\n",
    "\n",
    "        self.act1 = nn.Tanh()\n",
    "        self.act2 = nn.Tanh()\n",
    "\n",
    "    def forward(self, x_embd, lengths):\n",
    "        packed = pack_padded_sequence(x_embd,\n",
    "                                      lengths,\n",
    "                                      batch_first=True,\n",
    "                                      enforce_sorted=False)\n",
    "        output, _ = self.sequence(packed)\n",
    "        output_unpacked, _ = pad_packed_sequence(output, batch_first=True)\n",
    "        output_norm = self.bn_enc(output_unpacked.permute(0, 2, 1))\n",
    "\n",
    "        clf1 = self.clf1(output_norm.permute(0, 2, 1))\n",
    "        clf1_norm = self.act1(self.bn_clf1(clf1.permute(0, 2, 1)))\n",
    "\n",
    "        clf2 = self.clf2(clf1_norm.permute(0, 2, 1))\n",
    "        clf2_norm = self.act1(self.bn_clf2(clf2.permute(0, 2, 1)))\n",
    "\n",
    "        return self.fc(clf2_norm.permute(0, 2, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Each task (word segmentation and char generation) has its loss function.\n",
    "##### To enhance the performance, the word segmentation layer is frozen until the char generation layer nearly converges to a solution. As the training loop may take a long time, the unfreezing occurs at loop number 450. This freezing/unfreezing approach allows the base sequence layer `self.encoder` to come with meaningful weights when training the word segmentation layer. Also, it prevents the gradients from the word segmentation layer from being a source of noise for the base sequence layer `self.encoder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X,\n",
    "          lengths,\n",
    "          y,\n",
    "          vocab_size,\n",
    "          emb_size,\n",
    "          enc_h_size,\n",
    "          seg_h_size,\n",
    "          gen_h_size,\n",
    "          batch_size,\n",
    "          epochs,\n",
    "          device,\n",
    "          model=None):\n",
    "\n",
    "    batches = Batcher(X,\n",
    "                      lengths,\n",
    "                      y,\n",
    "                      device,\n",
    "                      batch_size=batch_size,\n",
    "                      max_iter=epochs)\n",
    "\n",
    "    if not model:\n",
    "        m = SingleDualObj(vocab_size, emb_size, enc_h_size,\n",
    "                          seg_h_size, gen_h_size).to(device)\n",
    "    else:\n",
    "        m = model\n",
    "\n",
    "    loss_seg = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "    loss_gen = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "    epoch = 0\n",
    "    # freeze segmentation layer\n",
    "    for layer in m.word_seg.children():\n",
    "        for parameter in layer.parameters():\n",
    "            parameter.requires_grad = False\n",
    "    \n",
    "    optimizer = optim.Adam(m.parameters(), lr=0.001)\n",
    "\n",
    "    for batch in batches:\n",
    "\n",
    "        tot_loss = 0\n",
    "        for X, ls, y in zip(*batch):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Unfreeze segmentation layer\n",
    "            if epoch == 449:\n",
    "                for layer in m.word_seg.children():\n",
    "                    for parameter in layer.parameters():\n",
    "                        parameter.requires_grad = True\n",
    "\n",
    "            o_gen, o_seg = m(X, ls)\n",
    "            l = loss_gen(o_gen.permute(0, 2, 1), X[:, 1:max(ls)])\n",
    "            if epoch >= 449:\n",
    "                l += loss_seg(o_seg.permute(0, 2, 1), y[:, :max(ls)])\n",
    "            \n",
    "            tot_loss += l.item()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if (epoch + 1) % 25 == 0 or epoch == 0:\n",
    "            print(\"Total loss in epoch {:<3d} is {:.3f}.\".format(\n",
    "                epoch, tot_loss))\n",
    "\n",
    "        epoch += 1\n",
    "\n",
    "    # print(\"Total loss in epoch {:<2d} is {:.3f}.\".format(epoch - 1, tot_loss))\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\n",
    "    \"vocab_size\": len(int_index),\n",
    "    \"emb_size\": 300,\n",
    "    \"enc_h_size\": 200,\n",
    "    \"seg_h_size\": 150,\n",
    "    \"gen_h_size\": 700,\n",
    "    \"batch_size\": 64,\n",
    "    \"epochs\": 500,\n",
    "    \"device\": torch.device(\"cuda:1\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss in epoch 0   is 385.279.\n",
      "Total loss in epoch 24  is 20.162.\n",
      "Total loss in epoch 49  is 18.570.\n",
      "Total loss in epoch 74  is 53.748.\n",
      "Total loss in epoch 99  is 18.481.\n",
      "Total loss in epoch 124 is 18.579.\n",
      "Total loss in epoch 149 is 18.107.\n",
      "Total loss in epoch 174 is 19.980.\n",
      "Total loss in epoch 199 is 17.854.\n",
      "Total loss in epoch 224 is 17.788.\n",
      "Total loss in epoch 249 is 17.272.\n",
      "Total loss in epoch 274 is 17.017.\n",
      "Total loss in epoch 299 is 37.176.\n",
      "Total loss in epoch 324 is 16.206.\n",
      "Total loss in epoch 349 is 15.990.\n",
      "Total loss in epoch 374 is 15.949.\n",
      "Total loss in epoch 399 is 15.516.\n",
      "Total loss in epoch 424 is 15.257.\n",
      "Total loss in epoch 449 is 67.710.\n",
      "Total loss in epoch 474 is 15.518.\n",
      "Total loss in epoch 499 is 15.130.\n"
     ]
    }
   ],
   "source": [
    "model = train(train_X_tensor, train_lengths_tensor, train_y_tensor, **kwargs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Use two separates lists to hold the ground truth  and the predections for each task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "loss = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "probs = []\n",
    "gen_preds = []\n",
    "gen_gtruth = []\n",
    "seg_preds = []\n",
    "seg_gtruth = []\n",
    "test_lengs = test_lengths_tensor.to(torch.device(\"cpu\"))\n",
    "for i, X_test in enumerate(test_X_tensor):\n",
    "    with torch.no_grad():\n",
    "        output = model(X_test.view(1, -1), test_lengs[i].view(-1))\n",
    "        probs.append(\n",
    "            loss(output[0].permute(0, 2, 1),\n",
    "                 X_test[1:test_lengs[i]].view(1, -1)).item())\n",
    "\n",
    "    gen_prediction = torch.argmax(F.log_softmax(output[0], 2), 2)\n",
    "    seg_prediction = torch.argmax(F.log_softmax(output[1], 2), 2)\n",
    "\n",
    "    gen_preds.extend(gen_prediction.float().view(-1).tolist())\n",
    "    gen_gtruth.extend(X_test[1:test_lengs[i]].float().tolist())\n",
    "\n",
    "    seg_preds.extend(seg_prediction.float().view(-1).tolist())\n",
    "    seg_gtruth.extend(test_y_tensor[i][:test_lengs[i]].float().tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next character prediction Report:\n",
      "\n",
      "Accuracy: 16.54%\n",
      "Min: 1.204\n",
      "Max: 16.440\n",
      "AVG: 9.823\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean\n",
    "\n",
    "accuracy = accuracy_score(gen_gtruth, gen_preds)\n",
    "\n",
    "print(\"Next character prediction Report:\\n\")\n",
    "print(f\"Accuracy: {accuracy:.2%}\")\n",
    "\n",
    "\n",
    "print(f\"Min: {min(probs):.3f}\\nMax: {max(probs):.3f}\\nAVG: {mean(probs):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Segmentation Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00       500\n",
      "         1.0       1.00      1.00      1.00       500\n",
      "         2.0       0.92      0.94      0.93     12012\n",
      "         3.0       0.89      0.87      0.88      7194\n",
      "\n",
      "    accuracy                           0.92     20206\n",
      "   macro avg       0.95      0.95      0.95     20206\n",
      "weighted avg       0.92      0.92      0.92     20206\n",
      "\n",
      "Accuracy: 91.68%\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(seg_gtruth, seg_preds)\n",
    "\n",
    "print(\"Word Segmentation Report:\\n\")\n",
    "print(classification_report(seg_gtruth, seg_preds))\n",
    "print(f\"Accuracy: {accuracy:.2%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
